{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Detector Effects Inducing Ambiguous $\\omega\\pi^0$ PWA Results?\n",
    "The results of the [detector effects](../detector_effects/detector_effects.ipynb) input-output study showed no clear indication of where the bin-to-bin inconsistencies are occurring in $\\omega\\pi^0$ partial wave analysis. No single detector effect appeared to be responsible for the problem, but it was clear that when using the *ideal* or \"thrown\" Monte Carlo data, there were no issues (aside from the expected fluctuations in negative reflectivity $\\varepsilon=-1$ waves). This suggests that the detector effects could be \"muddying\" the likelihood to such a degree that an unambiguous minimum is no longer obtainable. The best way to check for ambiguities is to project the partial wave results to **Moments**. \n",
    "\n",
    "In short, moments $H$ are the expansion coefficients of orthogonal basis functions \n",
    "$$\n",
    "    I^\\alpha(\\Omega,\\Omega_H) = \n",
    "        \\sum_{J_v=0}^2\n",
    "        \\sum_{\\Lambda=-2}^{2}\n",
    "        \\sum_{J=0}^{2*\\text{max}(J_i)}\n",
    "        \\sum_{M=-2*\\text{max}(m_i)}^{2*\\text{max}(m_i)}\n",
    "        \\frac{2J+1}{4\\pi} \\frac{2J_v+1}{4\\pi}\n",
    "        H^\\alpha(J_v,\\Lambda,J,M) \n",
    "        D_{M,\\Lambda}^{J\\ast}(\\Omega) D_{\\Lambda,0}^{J_v\\ast}(\\Omega_H)\n",
    "        \\,,\n",
    "$$\n",
    "here the Wigner D-functions $D_{m,n}^i(\\theta,\\phi,0)$, that describe our intensity. Though they lack the physical interpretation that partial waves provide, they are unique by construction i.e. do not suffer from ambiguous solutions. Moments can be written in terms of spin density matrix elements $\\rho_{i,j,m_i,m_j}$ as\n",
    "\\begin{align*}\n",
    "H^\\alpha(J_v,\\Lambda,J,M) = \n",
    "        &\\sum_{i=0}^{\\text{max}(J\\ell)_i}\n",
    "        \\sum_{j=0}^{\\text{max}(J\\ell)_j}\n",
    "        \\sum_{m_i=-J_i}^{J_i}\n",
    "        \\sum_{m_j=-J_j}^{J_j}\n",
    "        \\sum_{\\lambda=-1}^1\n",
    "        \\sum_{\\lambda'=-1}^1\n",
    "        \\frac{1}{2J_j+1} \\frac{1}{3}\n",
    "        \\\\\n",
    "        &\\times\n",
    "        \\braket{\\ell_i,0;1,\\lambda|J_i,\\lambda}\n",
    "        \\braket{\\ell_j,0;1,\\lambda'|J_j,\\lambda'}\n",
    "        \\\\\n",
    "        &\\times\n",
    "        \\braket{1,\\lambda;J_v,\\Lambda|1,\\lambda'}\n",
    "        \\braket{1,0;J_v,0|1,0}        \n",
    "        \\\\\n",
    "        &\\times \n",
    "        \\braket{J_i,m_i;J,M|J_j,m_j}\n",
    "        \\braket{J_i,\\lambda;J,\\Lambda|J_j,\\lambda'}\n",
    "        \\rho_{i,j,m_i,m_j}^\\alpha\n",
    "        \\Psi^i(w)\\Psi^{j\\ast}(w)\n",
    "    \\,,\n",
    "\\end{align*}\n",
    "which are composed of the PWA complex production coefficients $[c]$ in the reflectivity basis\n",
    "\\begin{align}    \n",
    "    \\rho^0_{i,j,m_i,m_j} &=\n",
    "    \\sum_\\varepsilon\n",
    "        [c^i]_{m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} +\n",
    "        (-1)^{m_i+m_j+\\ell_i+\\ell_j+J_i+J_j}\n",
    "        [c^i]_{-m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "    \\,,\n",
    "    \\\\          \n",
    "    \\rho^1_{i,j,m_i,m_j} &=\n",
    "    \\sum_\\varepsilon\n",
    "        \\varepsilon \\left(\n",
    "            (-1)^{1+m_i+\\ell_i+J_i}\n",
    "            [c^i]_{-m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} +\n",
    "            (-1)^{1+m_j+\\ell_j+J_j}\n",
    "            [c^i]_{m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "        \\right)\n",
    "    \\,,    \n",
    "    \\\\  \n",
    "    \\rho^2_{i,j,m_i,m_j} &= i\n",
    "    \\sum_\\varepsilon\n",
    "        \\varepsilon \\left(\n",
    "            (-1)^{m_i+\\ell_i+J_i}\n",
    "            [c^i]_{-m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} -\n",
    "            (-1)^{m_j+\\ell_j+J_j}\n",
    "            [c^i]_{m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "        \\right)  \n",
    "    \\,.    \n",
    "\\end{align}\n",
    "For a more detailed explanation of these moments and how they can be obtained from the original partial wave description of the intensity, see [the linked note](https://halldweb.jlab.org/doc-private/DocDB/ShowDocument?docid=6715). When ambiguities are present this effectively means different combinations of partial wave values can reproduce the same intensity value, and thus the same moments. The core problem is one of an underdetermined system of equations, in which there are simply more free parameters (partial waves) than equations to constrain them. By projecting out the moments for two partial wave fit results, we can determine whether or not they are part of the same unique solution, and thus ambiguous.\n",
    "\n",
    "This study will have a very similar structure to the detector effects study, where a \"truth\" file for each detector effect that contains the true generated values is used as a comparison point for \"truth-initialized\" fits, or a fit whose parameters are initialized to the generated values. This study will project out the moment values from the truth and truth-initialized results in each detector effect situation. If the fit result agrees with the truth values, such as in the *ideal* case, then there is no concern, as its clear the fit is able to arrive at the true solution. If the partial wave fits do not agree, then we can expect the following two outcomes:\n",
    "\n",
    "1. **The projected moment values agree** and thus our each fit is one of potentially many ambiguous solutions. As stated before, differing partial wave results with identical moments means that we have found a case in which the same intensity value can be described by different sets of partial wave values.\n",
    "2. **The projected moment values disagree** and so we can conclude that our fit is diverging towards a uniquely different solution from the true one. \n",
    "\n",
    "Both situations are troubling. The only way to solve the ambiguities of case 1 would be to add constraints e.g. mass-dependent functions and/or removing waves. Case 2 does not rule out whether the true minima has ambiguities, and tells us that for some odd reason the fit diverges to a wrong result, despite being initialized right at the true set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Details\n",
    "At its core, this is an input-output study. The input is [GlueX $\\omega\\pi^0$ Monte Carlo](https://halldweb.jlab.org/wiki-private/index.php/Omega_Pi_Simulation_Samples_Version_3#Neutral_signal_versionsver3.1) signal (`ver3.1`) and phasespace (`ver03`) files, that are passed through a DSelector which turns on/off several detector effects. The output is the partial wave fits, whose values are initialized to the \"true\" values that generated the input, and the corresponding moments projected from those fits.\n",
    "\n",
    "### Input\n",
    "We have the following effects available to study:\n",
    "1. **No effects (referred to as *thrown* or *ideal*):** This is the \"perfect detector\" scenario where all tracks are detected, reconstructed, and identified exactly\n",
    "2. **Acceptance Effects:** In this case the detector acceptance is applied, and so we only use tracks that are detected in the simulation. Note that the 4-momenta still match their generated values\n",
    "3. **Reconstruction (or *matched*):** Now we apply the reconstruction algorithm, and so our 4 momenta now have some resolution effects applied to them. This is called the *matched* case because we are still using perfect identification of the particles i.e. we know precisely which two photons were produced from the $\\pi^0$ in the $\\omega\\rightarrow \\pi^+\\pi^-\\pi^0$ decay, which otherwise could have been confused with the $\\pi^0$ in the $X\\rightarrow \\omega\\pi^0$ decay. We do this by \"matching\" the generated to the reconstructed tracks for the photons. In this way we use the reconstructed values but have no mis-identification possibilities\n",
    "   1. Note that we are only matching the $\\pi^0$'s, but in theory the $\\pi^+$ from the $\\omega$ could be confused with the recoil proton. This is highly unlikely to occur though, and so is of no concern.\n",
    "4. **All Effects:** This final step applies the detector effects, reconstruction algorithm, and allows for $\\pi^0$ combinatorics to occur, thus providing the closest approximation of real data as possible.\n",
    "\n",
    "In addition to the above, every one of these steps has an additional effect we can tack on: the out-of-time beam photons. When we use any \"no-accidental\" data, it means that our beam photon 4-momenta comes from the exact photon that generated the event. This is an idealized case, and we can instead simulate real data by having a set of out-of-time photons lie under the prompt in-time peak. To best determine what photon generated the event, we will then have to use the RF sideband subtraction method to remove these out-of-time photons, which is of course not perfect and can cause us to use the wrong photon. In short, any dataset labelled \"noaccidental\" uses the precise beam photon that generated the event, and datasets without this flag use RF sideband subtraction.\n",
    "\n",
    "All together we have 8 datasets: 4 simulated effects + with/without RF sideband subtraction for each.\n",
    "\n",
    "\n",
    "For any of these detector scenarios, the data is separated into bins of $\\omega\\pi^0$ mass 20 MeV wide, from 1.0 - 2.0 GeV. It uses the coherent peak energy range (8.2 - 8.8 GeV) and selects events within the the four moment transfer window $0.3 < -t < 0.5~GeV^2$. There is nothing particularly special about this window, but was simply chosen to limit the dataset file sizes. \n",
    "\n",
    "\n",
    "#### Getting the truth information\n",
    "We'd like to compare how well our output aligns with our true generated values, but obtaining those true values is not so simple. As discussed, this input comes from a mass-dependent fit, causing 2 problems\n",
    "1. The overall scale of the production parameters are sensitive to the total intensity i.e. the number of events, but the input data is generated with a different number of events than the we originally fit to, and these events are further modified by detector effects\n",
    "2. The production parameters are constant for the entire mass range, and are modulated continuously by the Breit-Wigners. We want the values for individual mass bins.\n",
    "We can alleviate both of these issues by performing a highly constrained fit in each mass bin using a `truth.cfg` file. This file fixes all the production coefficients to their generated values, but multiplies them all by an `intensity_scale` factor, which is left floating in the fit. This forces the fitter to keep the same interference behavior and allow the fit to adjust to any overall intensity, thus addressing Problem 1. The data is partitioned already into bins of 20 MeV, and so by running these fits with this config, the included Breit-Wigner functions will properly adjust the production coefficients for this narrow mass bin, handling Problem 2. This provides us with a set of properly scaled production coefficients, and therefore amplitudes, to compare with in each mass bin for any detector scenario. The moments can also be projected from these production coefficients, giving us a set of \"truth moments\" that we can compare our output \"fit moments\" to.\n",
    "\n",
    "### Output\n",
    "The output we want to analyze will be the results of our truth-initialized fits. The truth-initialized fits are nearly identical to the `truth.cfg` file, but here we are letting the production parameters (which are initialized to the true values) float and fixing the `intensity_scale` parameter to the value obtained by the truth fit for that bin. In other words, we are starting the fit off at the true set of production coefficients, and we use the result of the truth fit to properly scale those coefficients for the number of events in that bin. By all expectations, we should be starting the fit off at the likelihood minimum and don't expect these truth-initialized fits to diverge from where they start. We can of course project out the moment values of these truth-initialized fits and compare them with our true moment values. As discussed in the introduction, if the amplitude outputs are different than the inputs, then the moments can be used to check for the presence of underconstrained ambiguities.\n",
    "\n",
    "#### Bootstrapping for uncertainties\n",
    "When comparing amplitude values, we often bootstrap the data to get a better approximation of the statistical errors compared to the MINUIT estimated error. This procedure is important for amplitudes, but necessary for moments, as propagating the MINUIT uncertainties from the production coefficients all the way to the moments is complicated. We can avoid this difficult propagation by instead projecting out the moments from the bootstrapped fits, thus providing an approximation on the error for the moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Pre-Processing\n",
    "\n",
    "### Packages\n",
    "Lets start by loading in some packages and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some typical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "# load in default matplotlib style\n",
    "plt.style.use(\"/w/halld-scshelf2101/kscheuer/neutralb1/analysis/scripts/pwa_plotter.mplstyle\")\n",
    "\n",
    "# load useful paths and functions\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "PARENT_DIR = str(Path().resolve().parents[2])\n",
    "WORKING_DIR = f\"{PARENT_DIR}/analysis/input-output-tests/moment-projection/\"\n",
    "sys.path.insert(0, PARENT_DIR)\n",
    "import analysis.scripts.pwa_tools as pwa_tools\n",
    "\n",
    "# Load the environment variables. This contains needed setup_gluex.sh variables e.g. $ROOTSYS, $HALLD_HOME\n",
    "import subprocess\n",
    "command = f\"bash -l -c 'source {PARENT_DIR}/setup_gluex.sh && env'\"\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, executable='/bin/bash')\n",
    "output, _ = proc.communicate()\n",
    "# Parse the environment variables\n",
    "env_vars = {}\n",
    "for line in output.decode().splitlines():       \n",
    "    # the output contains a bunch of BASH_FUNCS that will ruin the environment variables. this avoids those issues\n",
    "    if len(line.split('=', 1)) != 2 or line.startswith(\"BASH_FUNC\") or line.startswith(\" \") or line.startswith(\"\\t\"):\n",
    "        continue    \n",
    "    key, value = line.split('=', 1)\n",
    "    env_vars[key] = value\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also setup a few other useful constants as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_list = [\"thrown\", \"accept_noacc\", \"accept\", \"matched_noacc\", \"matched\", \"all_noacc\", \"all_effects\"]\n",
    "# when showing scenarios on same plots, we'll want a consistent color and marker scheme\n",
    "SCENARIO_COLORS = dict(zip(scenario_list, sns.color_palette(\"deep\", len(scenario_list))))\n",
    "SCENARIO_MARKERS = dict(zip(scenario_list, [\"o\", \"s\", \"D\", \"P\", \"X\", \"v\", \"^\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading fit results\n",
    "The following cell is in raw mode to avoid unnecessary execution, and is kept here as a reminder of what flags were used to submit the fits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%run -i $parent_dir/submission/batch_scripts/submit.py -w 1p 1m iso -o PARA_0 -m 1.00 2.00 0.02 -t 0.3 0.5 0.2 --data-version ver03.1 --data-option _mcthrown --phasespace-version ver03, --phasespace-option  -c 0.0 -g 1 A100 --time-limit 0:30:00 --truth-file truth_ver03.1.cfg --email kscheuer@jlab.org --email-type FAIL\n",
    "# replace \"mcthrown\" in --data_option for each case (_accept, _matched, \"\") + _noaccidental option for each\n",
    "# replace the phasespace-option for each detector case (_accept, _matched, \"\"). \n",
    "#     Remember the thrown case uses the gen phasepace \n",
    "#     The phasespace also has an implicit \"noaccidental\" flag built into it, and so can be used for both cases e.g. matched & matched_noaccidental\n",
    "# replace \"truth_ver03.1.cfg\" with \"truth__init_ver03.1.cfg\" to do the separate truth initialized fits\n",
    "# add a \"-b 100\" for 100 bootstrap fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now convert all the results into the needed csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tempfile\n",
    "def collect_csv_files(parent_path:str, scenario:str) -> None:\n",
    "    \"\"\"Aggregate all the different csv files for a given scenario\n",
    "\n",
    "    Makes a csv for the the truth, fit, and data files, and the moments of the truth and fit files.\n",
    "    Also make fit and moment csv files for the bootstrap fits.\n",
    "    parent_path must end at the truth subdirectory\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure the parent path is valid\n",
    "    parent_path += \"/\" if not parent_path.endswith(\"/\") else \"\"\n",
    "    parent_path = os.path.expanduser(parent_path) # expand the ~\n",
    "    if not parent_path.endswith(\"truth/\"):\n",
    "        raise ValueError(\"parent_path must end at the truth subdirectory\")\n",
    "    \n",
    "    # path to the conversion script and common output directory\n",
    "    script = f\"{PARENT_DIR}/analysis/scripts/collect_csv.py\"\n",
    "    output_dir = f\"{WORKING_DIR}{scenario}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        raise FileNotFoundError(f\"Directory {output_dir} does not exist\")\n",
    "\n",
    "    # input path for each file\n",
    "    input_files = [\n",
    "        \"best.csv\", # truth\n",
    "        \"init/best.csv\", # fits\n",
    "        \"data.csv\", # data\n",
    "        \"best_moments_table.csv\", # production coefficient table\n",
    "        \"best_moments.csv\", # truth moments\n",
    "        \"init/best_moments.csv\", # fit moments        \n",
    "        \"init/bootstrap/bootstrap.csv\", # bootstrap fits\n",
    "        \"init/bootstrap/bootstrap_moments.csv\", # bootstrap moments        \n",
    "    ]\n",
    "\n",
    "    output_names = [ # in same order as input_files\n",
    "        \"truth.csv\",\n",
    "        \"fit.csv\",\n",
    "        \"data.csv\",\n",
    "        \"table.csv\",\n",
    "        \"truth_moments.csv\",        \n",
    "        \"fit_moments.csv\",        \n",
    "        \"bootstrap.csv\",\n",
    "        \"bootstrap_moments.csv\",\n",
    "    ]\n",
    "    # create csvs if they don't exist\n",
    "    for f, name in zip(input_files, output_names):        \n",
    "        if not os.path.exists(f\"{output_dir}/{name}\"):    \n",
    "            full_input = glob.glob(f\"{parent_path}{f}\") # expand the wildcards\n",
    "            # use a tempfile to store all the files to not overload the subprocess command\n",
    "            with tempfile.NamedTemporaryFile(delete=False, mode='w') as temp_file:\n",
    "                temp_file.write(\"\\n\".join(full_input))\n",
    "                temp_file_path = temp_file.name                       \n",
    "            \n",
    "            subprocess.run([\"python\", script, \"-i\", temp_file_path, \"-o\", f\"{output_dir}/{name}\"])            \n",
    "        else:\n",
    "            print(f\"{output_dir}/{name} already exists, skipping\")\n",
    "\n",
    "    return\n",
    "\n",
    "common_path = (\n",
    "    \"~/volatile/ampToolsFits/omegapi/allPeriods/PARA_0/ver03.1_mcSCENARIO/\"\n",
    "    \"ver03OPTION/1m_1p_iso/recoil-pi-mass_0.0/t_0.30-0.50/*/truth/\"\n",
    ")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"thrown\").replace(\"OPTION\", \"\"), \"thrown\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"accept_noaccidental\").replace(\"OPTION\", \"_accept\"), \"accept_noaccidental\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"accept\").replace(\"OPTION\", \"_accept\"), \"accept\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"matched_noaccidental\").replace(\"OPTION\", \"_matched\"), \"matched_noaccidental\")\n",
    "# collect_csv_files(common_path.replace(\"SCENARIO\", \"matched\").replace(\"OPTION\", \"_matched\"), \"matched\")\n",
    "# collect_csv_files(common_path.replace(\"SCENARIO\", \"noaccidental\").replace(\"OPTION\", \"\"), \"all_noaccidental\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"\").replace(\"OPTION\", \"\"), \"all_effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then load all of these into pandas dataframes. Pandas will interpret the moment csv's to be strings, so we need to ensure they're interpreted as complex values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(subdir:str) -> Dict[str, pd.DataFrame]:\n",
    "    if not subdir.endswith(\"/\"):\n",
    "        subdir += \"/\"\n",
    "\n",
    "    # load dataframes. Assumes files all in same format for any subdir\n",
    "    truth = pd.read_csv(f\"{subdir}truth.csv\")\n",
    "    truth_moments = pd.read_csv(f\"{subdir}truth_moments.csv\")\n",
    "    fit = pd.read_csv(f\"{subdir}fit.csv\")\n",
    "    fit_moments = pd.read_csv(f\"{subdir}fit_moments.csv\")\n",
    "    bootstrap = pd.read_csv(f\"{subdir}bootstrap.csv\")\n",
    "    bootstrap_moments = pd.read_csv(f\"{subdir}bootstrap_moments.csv\")\n",
    "    data = pd.read_csv(f\"{subdir}data.csv\")\n",
    "    table = pd.read_csv(f\"{subdir}table.csv\")\n",
    "\n",
    "    # make sure moment dataframes register as complex values (except for the file column)\n",
    "    truth_moments.loc[:, truth_moments.columns != \"file\"] = truth_moments.loc[:, truth_moments.columns != \"file\"].astype(complex)\n",
    "    fit_moments.loc[:, fit_moments.columns != \"file\"] = fit_moments.loc[:, fit_moments.columns != \"file\"].astype(complex)\n",
    "    bootstrap_moments.loc[:, bootstrap_moments.columns != \"file\"] = bootstrap_moments.loc[:, bootstrap_moments.columns != \"file\"].astype(complex)    \n",
    "\n",
    "    df_dict = {\n",
    "        \"truth\" : truth,\n",
    "        \"truth_moments\" : truth_moments,\n",
    "        \"fit\" : fit,\n",
    "        \"fit_moments\" : fit_moments,\n",
    "        \"bootstrap\" : bootstrap,\n",
    "        \"bootstrap_moments\" : bootstrap_moments,\n",
    "        \"data\" : data,\n",
    "        \"table\": table\n",
    "    }\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "# Load all the dataframes\n",
    "thrown_dfs = load_dataframes(f\"{WORKING_DIR}thrown/\")\n",
    "accept_noacc_dfs = load_dataframes(f\"{WORKING_DIR}accept_noaccidental/\")\n",
    "accept_dfs = load_dataframes(f\"{WORKING_DIR}accept/\")\n",
    "# matched_noacc_dfs = load_dataframes(f\"{WORKING_DIR}matched_noaccidental/\")\n",
    "# matched_dfs = load_dataframes(f\"{WORKING_DIR}matched/\")\n",
    "# all_noacc_dfs = load_dataframes(f\"{WORKING_DIR}all_noaccidental/\")\n",
    "all_effect_dfs = load_dataframes(f\"{WORKING_DIR}all_effects/\")\n",
    "\n",
    "all_dfs = {\n",
    "    \"thrown\": thrown_dfs,\n",
    "    \"accept_noacc\": accept_noacc_dfs,\n",
    "    \"accept\": accept_dfs,\n",
    "    # \"matched_noacc\": matched_noacc_dfs,\n",
    "    # \"matched\": matched_dfs,\n",
    "    # \"all_noacc\": all_noacc_dfs,\n",
    "    \"all_effects\": all_effect_dfs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Before we get into any real analysis, we'll want to run some common checks on our moments to ensure that they're projection from amplitudes $\\rightarrow$ moments ran as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real & Imaginary Parts\n",
    "First let's check that the imaginary(real) parts of the $H^0,H^1$($H^2$) moments are zero as we expect. Lets plot these components as a function of mass for each file's truth and fit moments. We'll plot them all together, even though the plot can get crowded, just to check that they all within some floating point error tolerance of zero. We can also use this chance to check that our truth and fit results have the same moment set as well i.e. number of rows and columns in the csv's match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in all_dfs.items():\n",
    "    if val[\"truth_moments\"].shape[1] != val[\"fit_moments\"].shape[1] or val[\"truth_moments\"].shape[1] != val[\"bootstrap_moments\"].shape[1]:\n",
    "        raise ValueError(f\"Number of moments do not match for {key}\")\n",
    "    if val[\"truth_moments\"].shape[0] != val[\"fit_moments\"].shape[0]:\n",
    "        raise ValueError(f\"Number of files does not match for {key}\")\n",
    "\n",
    "    h0_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H0\")]\n",
    "    h1_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H1\")]\n",
    "    h2_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H2\")]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    mass_bins = val[\"data\"][\"m_center\"]\n",
    "    bin_width = (val[\"data\"][\"m_high\"] - val[\"data\"][\"m_low\"])[0]\n",
    "\n",
    "    # plot truth moments with lines and fit moments as points\n",
    "    val[\"truth_moments\"][h0_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"-\", color=\"blue\", alpha=0.8, legend=False)\n",
    "    val[\"fit_moments\"][h0_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"\", marker=\"o\", color=\"blue\", alpha=0.4, legend=False)\n",
    "    val[\"truth_moments\"][h1_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"--\", alpha=0.8, color=\"orange\", legend=False)\n",
    "    val[\"fit_moments\"][h1_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"\", marker=\"s\", color=\"orange\", alpha=0.4, legend=False)\n",
    "    val[\"truth_moments\"][h2_columns].map(lambda x: x.real).plot(ax=ax, linestyle=\":\", alpha=0.8, color=\"green\", legend=False)\n",
    "    val[\"fit_moments\"][h2_columns].map(lambda x: x.real).plot(ax=ax, linestyle=\"\", marker=\"D\", color=\"green\", alpha=0.4, legend=False)\n",
    "\n",
    "    # create some dummy lines for a nice legend\n",
    "    h0_dummy = mpl.lines.Line2D([0], [0], linestyle=\"-\", color=\"blue\", label=r\"$\\Im(H^0)$\")\n",
    "    h1_dummy = mpl.lines.Line2D([0], [0], linestyle=\"--\", color=\"orange\", label=r\"$\\Im(H^1)$\")\n",
    "    h2_dummy = mpl.lines.Line2D([0], [0], linestyle=\":\", color=\"green\", label=r\"$\\Re(H^2)$\")\n",
    "\n",
    "    ax.legend(handles=[h0_dummy, h1_dummy, h2_dummy])\n",
    "\n",
    "    print(f\"Plotting {key}\")\n",
    "    ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "    ax.set_ylabel(f\"Value / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding non-zero moments\n",
    "For our analysis we're only interested in those moments who are non-zero, so we'll use the truth moments to determine this. We'll also run a check to make sure the fit projected moments have the same set of non-zero moments as the truth moments for that scenario, and the truth moments of all other scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-10\n",
    "\n",
    "nonzero_columns = []\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    # get the nonzero columns for each scenario\n",
    "    moment_columns = [col for col in df_dict[\"truth_moments\"] if col.startswith(\"H\")]\n",
    "    scenario_nonzero_columns = [col for col in moment_columns if (df_dict[\"truth_moments\"][col].abs() > tolerance).any()]\n",
    "\n",
    "    # check if the nonzero columns are the same for each scenario\n",
    "    if not nonzero_columns:\n",
    "        nonzero_columns = scenario_nonzero_columns\n",
    "    else:\n",
    "        if set(nonzero_columns) != set(scenario_nonzero_columns):\n",
    "            print(f\"Scenario {scenario} has different nonzero moments\")\n",
    "            print(set(nonzero_columns) - set(scenario_nonzero_columns))\n",
    "\n",
    "    # check if the nonzero columns are the same between truth and fit\n",
    "    fit_cols = [col for col in moment_columns if (df_dict[\"fit_moments\"][col].abs() > tolerance).any()]\n",
    "    if set(scenario_nonzero_columns) != set(fit_cols):\n",
    "        print(f\"Scenario {scenario} has different nonzero moments between truth and fit\")        \n",
    "        print(set(nonzero_columns) - set(fit_cols))\n",
    "        \n",
    "print(f\"Reducing moments for all dataframes to {len(nonzero_columns)} nonzero moment columns\")\n",
    "for col in nonzero_columns:\n",
    "    print(col)\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    for key in [\"truth_moments\", \"fit_moments\", \"bootstrap_moments\"]:\n",
    "        df_dict[key] = df_dict[key][[\"file\"] + nonzero_columns]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting errors from the bootstrap distributions\n",
    "The bootstrap values provide a better approximation of the error for the amplitudes. In the case of the moments, we cannot easily propagate the MINUIT production coefficient errors to the moments, and so the bootstrap provides the only error. Before using them, we want to make sure they're roughly gaussian distributed by using a [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) and ensuring that its resultant p-values are greater than 0.05. \n",
    "\n",
    "Distributions that do not pass this tests are plotted on a probability plot, grouped by mass bin and scenario, and saved to a pdf. These probability plots, made using `scipy.stats.probplot`, are the same as Q-Q plots [(despite what the documentation implies)](https://stackoverflow.com/questions/48108582/how-to-interpret-scipy-stats-probplot-results). Deviations from the best fit line indicate that the bootstrap distribution is not similar to a normal distribution, and we should be wary of using its standard deviation as an estimation of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "#TODO: remove amplitude ones, since we just want production coefficients, but those need adjustment by breit wigners\n",
    "def shapiro_test(grouped_df:pd.DataFrame, columns:list, pdf: PdfPages) -> None:\n",
    "    # test the normality of the bootstrap data, grouped by file name\n",
    "\n",
    "    # use dict of dicts to store { col : {scenario : p-value} } for distributions that are not normal\n",
    "    non_normal_dict = {}\n",
    "    for col in columns:     \n",
    "        for scenario in grouped_df['scenario'].unique():   \n",
    "            stat, p_value = scipy.stats.shapiro(grouped_df[grouped_df['scenario'] == scenario][col])        \n",
    "            if p_value < 0.05:\n",
    "                non_normal_dict.setdefault(col, {})[scenario] = p_value\n",
    "\n",
    "    # Plot the non-normal columns\n",
    "    if non_normal_dict:\n",
    "        # setup the subplots to be a square grid\n",
    "        num_plots = len(non_normal_dict)\n",
    "        num_cols = math.ceil(math.sqrt(num_plots))\n",
    "        num_rows = math.ceil(num_plots / num_cols)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))\n",
    "        axes = axes.flatten() if num_plots > 1 else [axes]\n",
    "        \n",
    "        # plot all non-normal scenarios on the same plot\n",
    "        for ax, (col, scenario_dict) in zip(axes, non_normal_dict.items()):\n",
    "            for scenario, p_value in scenario_dict.items():\n",
    "                scipy.stats.probplot(grouped_df[grouped_df['scenario'] == scenario][col], dist=\"norm\", plot=ax)\n",
    "\n",
    "                if any(col.startswith(\"H(\") for col in columns):\n",
    "                    ax.set_title(f\"{col}\")\n",
    "                else:\n",
    "                    ax.set_title(pwa_tools.convert_amp_name(col)) # make title be interpretable for amplitudes\n",
    "                    \n",
    "                best_fit_line = ax.get_lines()[-1] # always plotted last\n",
    "                data_markers = ax.get_lines()[-2]\n",
    "                \n",
    "                # change markers and best fit line to a color for that scenario\n",
    "                best_fit_line.set_color(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_markerfacecolor(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_markeredgecolor(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_marker(SCENARIO_MARKERS[scenario])\n",
    "                data_markers.set_label(f\"{scenario}:{p_value:.1e}\")\n",
    "                data_markers.set_alpha(0.7)\n",
    "                ax.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "        # create a figure legend explaining the scenario colors\n",
    "        handles = [\n",
    "            mpl.patches.Patch(color=color, label=scenario) \n",
    "            for scenario, color in SCENARIO_COLORS.items()\n",
    "        ]\n",
    "        fig.legend(handles=handles, loc='upper right', title='Scenarios', ncol=3)\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for ax in axes[num_plots:]:\n",
    "            ax.set_visible(False)\n",
    "        \n",
    "        fig.suptitle(f\"mass bin = {grouped_df.name}\", fontsize=20)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # give some more room for the title at the top\n",
    "        pdf.savefig(fig)\n",
    "        plt.close()\n",
    "\n",
    "# setup dataframes to hold all the bootstrap data, with their scenarios labelled\n",
    "all_bootstrap = pd.DataFrame()\n",
    "all_bootstrap_moment = pd.DataFrame()       \n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    # use dataframe copies to avoid modifying the original dataframes\n",
    "    copy_df = df_dict[\"bootstrap\"].copy()\n",
    "    copy_moment_df = df_dict[\"bootstrap_moments\"].copy()\n",
    "    copy_df[\"scenario\"] = scenario\n",
    "    copy_moment_df[\"scenario\"] = scenario    \n",
    "    \n",
    "    all_bootstrap = pd.concat([all_bootstrap, copy_df])\n",
    "    all_bootstrap_moment = pd.concat([all_bootstrap_moment, copy_moment_df])\n",
    "\n",
    "# Get columns for each dataframe\n",
    "# NOTE: only doing amplitudes for now, because unsure how to check for normality of circular data\n",
    "amp_dict = pwa_tools.get_coherent_sums(all_bootstrap)\n",
    "amp_columns = [col for sublist in amp_dict.values() for col in sublist] \n",
    "moment_columns = [col for col in all_bootstrap_moment.columns if col != \"file\" and col != \"scenario\"]\n",
    "for col in moment_columns: # ensure that moment columns are single values (real or imaginary)\n",
    "    if \"H0\" in col or \"H1\" in col:                \n",
    "        all_bootstrap_moment[col] = all_bootstrap_moment[col].map(lambda x: x.real)\n",
    "    elif \"H2\" in col:\n",
    "        all_bootstrap_moment[col] = all_bootstrap_moment[col].map(lambda x: x.imag)  \n",
    "\n",
    "# create dataframes grouped by the mass bin in the file name    \n",
    "all_bootstrap[\"mass\"] = all_bootstrap[\"file\"].str.extract(r\"/mass_([\\d.]+-[\\d.]+)\")    \n",
    "all_bootstrap_moment[\"mass\"] = all_bootstrap_moment[\"file\"].str.extract(r\"/mass_([\\d.]+-[\\d.]+)\")\n",
    "grouped_df = all_bootstrap.groupby(\"mass\")\n",
    "grouped_moment_df = all_bootstrap_moment.groupby(\"mass\")\n",
    "\n",
    "# apply the shapiro test to the grouped dataframes, and save all figs into their respective pdfs\n",
    "with PdfPages(\"shapiro_bootstrap.pdf\") as pdf:\n",
    "    # NOTE: only doing amplitudes for now, because unsure how to check for normality of circular data\n",
    "    amp_dict = pwa_tools.get_coherent_sums(all_bootstrap)    \n",
    "    grouped_df.apply(\n",
    "        shapiro_test, \n",
    "        columns=amp_columns, pdf=pdf,\n",
    "        include_groups=False\n",
    "    )\n",
    "    print(f\"Shapiro test results for bootstrapped amplitudes saved to: {os.path.join(WORKING_DIR, 'shapiro_bootstrap.pdf')}\")\n",
    "with PdfPages(\"shapiro_bootstrap_moments.pdf\") as pdf:    \n",
    "    \n",
    "    grouped_moment_df.apply(\n",
    "        shapiro_test, \n",
    "        columns=moment_columns, pdf=pdf, \n",
    "        include_groups=False\n",
    "    )\n",
    "    print(f\"Shapiro test results for bootstrapped moments saved to: {os.path.join(WORKING_DIR, 'shapiro_bootstrap_moments.pdf')}\")\n",
    "\n",
    "# Explicitly delete the dataframes we copied and created\n",
    "del all_bootstrap\n",
    "del all_bootstrap_moment\n",
    "del grouped_df\n",
    "del grouped_moment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $H^0(0,0,0,0)$ =? Generated Events\n",
    "The next check we can perform is to see how close the $H^0(0,0,0,0)$ moment is to the number of generated events. If all factors are handled appropriately these two should match. Even if they don't, we will normalize them soon. Below we'll plot the truth and fit $H^0(0,0,0,0)$ moments with the \\# of generated events, and what scale factor these are off by (averaged over the mass bins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in all_dfs.items():\n",
    "    h0_truth = val[\"truth_moments\"][\"H0(0,0,0,0)\"].map(lambda x: x.real)\n",
    "    h0_fit = val[\"fit_moments\"][\"H0(0,0,0,0)\"].map(lambda x: x.real)\n",
    "    generated_events = val[\"truth\"][\"generated_events\"]\n",
    "\n",
    "    scale_truth = (h0_truth / generated_events).mean()\n",
    "    scale_fit = (h0_fit / generated_events).mean()    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    mass_bins = val[\"data\"][\"m_center\"]\n",
    "    bin_width = (val[\"data\"][\"m_high\"] - val[\"data\"][\"m_low\"])[0]\n",
    "    ax.bar(mass_bins, generated_events, width=bin_width, color=\"gray\", alpha=0.5, label=\"Generated Events\")    \n",
    "    ax.plot(mass_bins, h0_truth, color=\"black\", marker=\"\", linestyle=\"-\", label=rf\"$H^0(0,0,0,0)_{{\\text{{truth}}}} (x{scale_truth:.2f})$\")\n",
    "    ax.plot(mass_bins, h0_fit, color=\"blue\", marker=\"o\", linestyle=\"\", label=rf\"$H^0(0,0,0,0)_{{\\text{{fit}}}} (x{scale_fit:.2f})$\")\n",
    "\n",
    "    print(f\"Plotting {key}\")\n",
    "    ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "    ax.set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")    \n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truth Moments Vs Detector Effect (with renormalization)\n",
    "Aside from an overall scale factor, we don't expect the truth moments to change for each detector effect. We can account for this scale factor by using the normalized moments \n",
    "$$\n",
    "\\hat{H}^\\alpha(J_v,\\Lambda,J,M) = \\frac{H^\\alpha(J_v,\\Lambda,J,M)}{H^0(0,0,0,0)}.\n",
    "$$ \n",
    "This also handles the fact that our fit and truth $H^0(0,0,0,0)$ moments do not match the number of generated events. We will continue to use these normalized moments for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_moments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # avoid normalizing already normalized data   \n",
    "    if (df[\"H0(0,0,0,0)\"].map(lambda x: x.real) == 1).all():               \n",
    "        return df\n",
    "    moment_cols = [col for col in df.columns if col.startswith(\"H\")]        \n",
    "    return df[moment_cols].div(df[\"H0(0,0,0,0)\"], axis=0)\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    for key, df in df_dict.items():\n",
    "        if \"moment\" not in key:\n",
    "            continue\n",
    "        df_dict[key] = normalize_moments(df)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check if our truth moments are identical for each detector effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = all_dfs[\"thrown\"][\"truth_moments\"].columns\n",
    "nonzero_h0 = [col for col in cols if col.startswith(\"H0\")]\n",
    "nonzero_h1 = [col for col in cols if col.startswith(\"H1\")]\n",
    "nonzero_h2 = [col for col in cols if col.startswith(\"H2\")]\n",
    "\n",
    "# setup a dataframe to store the truth moments for each scenario\n",
    "truth_df = pd.DataFrame()\n",
    "\n",
    "for scenario, df in all_dfs.items():\n",
    "    # get the nonzero real(imaginary) parts of the H0,H1,(H2) moments\n",
    "    h0_truth = df[\"truth_moments\"][nonzero_h0].map(lambda x: x.real)\n",
    "    h1_truth = df[\"truth_moments\"][nonzero_h1].map(lambda x: x.real)\n",
    "    h2_truth = df[\"truth_moments\"][nonzero_h2].map(lambda x: x.imag)\n",
    "\n",
    "    # add those truth moments to the dataframe with the mass and scenario\n",
    "    df = pd.concat([df[\"data\"][\"m_center\"], h0_truth, h1_truth, h2_truth], axis=1)\n",
    "    df.rename(columns={\"m_center\": \"mass\"}, inplace=True)\n",
    "    df[\"scenario\"] = scenario\n",
    "    truth_df = pd.concat([truth_df, df])\n",
    "\n",
    "# plot the relative truth moments for each scenario\n",
    "truth_df_melted = truth_df.melt(id_vars=[\"mass\", \"scenario\"], var_name=\"moment\", value_name=\"truth\")\n",
    "grid = sns.relplot(\n",
    "    data=truth_df_melted, x=\"mass\", y=\"truth\", col=\"moment\", \n",
    "    col_wrap=10, kind=\"line\", hue=\"scenario\", style=\"scenario\", facet_kws={'sharey': False}\n",
    ")\n",
    "\n",
    "for ax in grid.axes.flat:\n",
    "    # remove the \"moment = \" from the title\n",
    "    subplot_title = ax.get_title()\n",
    "    ax.set_title(subplot_title.replace(\"moment = \", \"\"))\n",
    "    # set the alpha value of each line\n",
    "    for line in ax.get_lines():\n",
    "        scenario = line.get_label()\n",
    "        if scenario == \"thrown\":\n",
    "            line.set_alpha(1.0)\n",
    "        else:\n",
    "            line.set_alpha(0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment $\\chi^2$/ndf and Mean Squared Error (MSE)\n",
    "(depending on Boris' answer, use these metrics to calculate these for each scenario. Then stitch them together with the JP plots for each scenario. The code can basically be copied from the residual difference part below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Difference\n",
    "With our data verified, we can move onto analysis. Lets plot the relative differences between the truth and fitted moments\n",
    "$$\n",
    "R =\n",
    "\\frac{\n",
    "    \\hat{H}^\\alpha(J_v,\\Lambda,J,M)_{\\text{fit}} - \\hat{H}^\\alpha(J_v,\\Lambda,J,M)_{\\text{truth}}\n",
    "}{\n",
    "    \\hat{H}^\\alpha(J_v,\\Lambda,J,M)_{\\text{truth}}\n",
    "}\n",
    "\\,.\n",
    "$$\n",
    "We use the relative difference (scaled by $H_{\\text{truth}}$) because we do not yet have access to the uncertainties of the fitted or truth moments, so this provides us with some sense of how far off the fit results are from the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the nonzero columns by alpha value\n",
    "cols = df_array[\"thrown\"][\"truth_moments\"].columns\n",
    "nonzero_h0 = [col for col in cols if col.startswith(\"H0\")]\n",
    "nonzero_h1 = [col for col in cols if col.startswith(\"H1\")]\n",
    "nonzero_h2 = [col for col in cols if col.startswith(\"H2\")]\n",
    "\n",
    "# setup a dataframe to store the relative differences R\n",
    "reldiff_df = pd.DataFrame()\n",
    "\n",
    "for scenario, df in df_array.items():\n",
    "    if scenario not in [\"thrown\", \"all_effects\"]:\n",
    "        continue\n",
    "    # get the nonzero real(imaginary) parts of the H0,H1,(H2) moments\n",
    "    h0_truth = df[\"truth_moments\"][nonzero_h0].map(lambda x: x.real)\n",
    "    h0_fit = df[\"fit_moments\"][nonzero_h0].map(lambda x: x.real)\n",
    "    h1_truth = df[\"truth_moments\"][nonzero_h1].map(lambda x: x.real)\n",
    "    h1_fit = df[\"fit_moments\"][nonzero_h1].map(lambda x: x.real)\n",
    "    h2_truth = df[\"truth_moments\"][nonzero_h2].map(lambda x: x.imag)\n",
    "    h2_fit = df[\"fit_moments\"][nonzero_h2].map(lambda x: x.imag)\n",
    "\n",
    "    # calculate the relative difference between the truth and fit moments\n",
    "    h0_diff = (h0_fit - h0_truth) / h0_truth\n",
    "    h1_diff = (h1_fit - h1_truth) / h1_truth\n",
    "    h2_diff = (h2_fit - h2_truth) / h2_truth    \n",
    "\n",
    "    # put those relative differences into a dataframe with the mass bins\n",
    "    diff_df = pd.concat([df[\"data\"][\"m_center\"], h0_diff, h1_diff, h2_diff], axis=1)\n",
    "    diff_df.rename(columns={\"m_center\": \"mass\"}, inplace=True)\n",
    "    diff_df[\"scenario\"] = scenario\n",
    "    reldiff_df = pd.concat([reldiff_df, diff_df])\n",
    "\n",
    "# plot the relative differences\n",
    "reldiff_df_melted = reldiff_df.melt(id_vars=[\"mass\", \"scenario\"], var_name=\"moment\", value_name=\"R\")\n",
    "grid = sns.relplot(\n",
    "    data=reldiff_df_melted, x=\"mass\", y=\"R\", col=\"moment\", \n",
    "    col_wrap=9, kind=\"line\", hue=\"scenario\", style=\"scenario\", facet_kws={'sharey': False}\n",
    ")\n",
    "\n",
    "\n",
    "for ax in grid.axes.flat:\n",
    "    ax.axhline(y=0, color='black', linestyle='-') # Add a horizontal line at y=0 to each plot\n",
    "    # remove the \"moment = \" from the title\n",
    "    subplot_title = ax.get_title()\n",
    "    ax.set_title(subplot_title.replace(\"moment = \", \"\"))\n",
    "    # set the alpha value of each line\n",
    "    for line in ax.get_lines():\n",
    "        scenario = line.get_label()\n",
    "        if scenario == \"thrown\":\n",
    "            line.set_alpha(1.0)\n",
    "        else:\n",
    "            line.set_alpha(1.0)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, but this is a lot of plots to look at. Let's sum the absolute values of the relative difference across the mass bins $M$\n",
    "$$\n",
    "    \\sum_M^i |R_i|\\,,\n",
    "$$\n",
    "for $N$ total mass bins to get a value that gives a sense of whether any detector effect is the main reason for the divergence of fit moments from the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a dataframe to store the sum of the absolute values of the relative differences for each scenario\n",
    "abs_sum_df = pd.DataFrame()\n",
    "\n",
    "scenarios = [\"thrown\", \"accept_noacc\", \"accept\", \"matched_noacc\", \"matched\", \"all_effects\"]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # get the relative differences for each scenario and their moment columns\n",
    "    scenario_df = reldiff_df[reldiff_df[\"scenario\"] == scenario]\n",
    "    moment_columns = [col for col in scenario_df.columns if col.startswith(\"H\")]\n",
    "\n",
    "    # sum the absolute values of those differences and sort them in descending order\n",
    "    sum_series = scenario_df[moment_columns].abs().sum()    \n",
    "    abs_sum_df = pd.concat([abs_sum_df, pd.DataFrame(sum_series, columns=[scenario])], axis=1)\n",
    "\n",
    "# transpose the dataframe so that the moments are the columns, and the scenarios are the rows\n",
    "abs_sum_df = abs_sum_df.T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "abs_sum_df.drop(\"H1(2,0,2,1)\", axis=1, inplace=True)\n",
    "\n",
    "abs_sum_df.plot(kind=\"line\", marker=\"o\", ax=ax, label=None, legend=None)\n",
    "ax.set_xticks(range(len(scenarios)))\n",
    "ax.set_xticklabels(scenarios)\n",
    "ax.set_xlabel(\"Scenario\")\n",
    "ax.set_ylabel(r\"$\\sum_M^N |R_M|$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below here is scratch work, stuff that I'll want to formerly put together above. I'll list some other todo stuff\n",
    "* Make plots like I did for detector_effects study, with mass distribution and a difference plot below it, but make that plot be something like the absolute scaled difference between moments $\\frac{H_{fit} - H_{gen}}{H_{gen}}$\n",
    "  * I'll have to use this instead of dividing by uncertainty because I can't propagate the partial wave errors yet\n",
    "  * Maybe have the project_moments script create another csv file that acts like a matrix, showing what values a partial wave contributes to a moment. I could use this for these plots when it comes to showing individual amplitudes, since using the scaled difference of *all* moments wouldn't be accurate as many of those moments would not have contributions from the amplitude being plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fit_df = pd.read_csv(\"../../../fits.csv\")\n",
    "moment_df = pd.read_csv(\"../../../moments.csv\", index_col=\"file\")\n",
    "\n",
    "print(moment_df[\"H0(0,0,0,0)\"].apply(lambda x: complex(x).real).max())\n",
    "print(fit_df[\"generated_events\"].max())\n",
    "scale = moment_df[\"H0(0,0,0,0)\"].apply(lambda x: complex(x).real).max() / fit_df[\"generated_events\"].max()\n",
    "print(scale)\n",
    "fit_df[\"generated_events\"].plot(label=\"gen events\")\n",
    "(fit_df[\"generated_events\"]*scale).plot(label=\"scaled events\")\n",
    "moment_df[\"H0(0,0,0,0)\"].apply(lambda x: complex(x).real).plot(label=\"H0(0,0,0,0)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the sum of the real and imaginary parts for each moment. We'll want to formarly do this in the final text because this is a good way to check that we projected things properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0\n",
    "for col in moment_df.columns:\n",
    "    if col.split(\"(\")[0][1] != alpha:\n",
    "        print(\"-\"*50)\n",
    "    alpha = col.split(\"(\")[0][1]\n",
    "    real_sum = moment_df[col].apply(lambda x: complex(x).real).sum()\n",
    "    imag_sum = moment_df[col].apply(lambda x: complex(x).imag).sum()    \n",
    "\n",
    "    print(f\"{col:<10}: {real_sum:<13.3f} + {imag_sum:.3f}i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick moment check for all effects case\n",
    "all_plotter = pwa_tools.Plotter(df_all_fit, df_all_data, truth_df=df_all_truth)\n",
    "all_plotter.jp(\"Recon MC\")\n",
    "all_plotter.intensities()\n",
    "all_plotter.intensities(True)\n",
    "all_plotter.matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# obtain moment columns (just remove the file column)\n",
    "moment_columns = df_all_truth_moments.columns[df_all_truth_moments.columns != \"file\"]\n",
    "\n",
    "df_all_truth_moments[h0_moments].applymap(lambda x: x.imag).plot()\n",
    "df_all_truth_moments[h1_moments].applymap(lambda x: x.imag).plot()\n",
    "df_all_truth_moments[h2_moments].applymap(lambda x: x.real).plot()\n",
    "\n",
    "# Rescale all the moments by the H0(0,0,0,0) moment. This appears to b needed in order to compare the truth to the fit moments\n",
    "df_scaled_truth = df_all_truth_moments[moment_columns].div(df_all_truth_moments[\"H0(0,0,0,0)\"], axis=0)\n",
    "df_scaled_fit = df_all_fit_moments[moment_columns].div(df_all_fit_moments[\"H0(0,0,0,0)\"], axis=0)\n",
    "\n",
    "# plot subtracted values scaled by truth values\n",
    "new_df = (df_scaled_truth - df_scaled_fit).div(df_scaled_truth.replace(0+0j, np.nan), axis=0).fillna(0+0j)\n",
    "new_df.plot(legend=None)\n",
    "plt.show()\n",
    "\n",
    "abs_sum = new_df.abs().sum(axis=1)\n",
    "\n",
    "sum_df = abs_sum / new_df.abs().count(axis=1)\n",
    "plt.plot(sum_df.index, sum_df)\n",
    "plt.show()\n",
    "\n",
    "# make a plot of the moments in the 11th and 12 bins, whose JP amplitudes are very far apart\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "\n",
    "# avoid the first scaled moment which is defined to be 1.0\n",
    "cols = [col for col in moment_columns if col != \"H0(0,0,0,0)\"]\n",
    "real_cols = [col for col in cols if col.startswith(\"H0\") or col.startswith(\"H1\")]\n",
    "imag_cols = [col for col in cols if col.startswith(\"H2\")]\n",
    "\n",
    "row1_truth = [x.real if col in real_cols else x.imag for col, x in zip(cols, df_scaled_truth.iloc[10][cols])]\n",
    "row1_fit = [x.real if col in real_cols else x.imag for col, x in zip(cols, df_scaled_fit.iloc[10][cols])]\n",
    "ax1.plot(cols, row1_truth, color=\"black\")\n",
    "ax1.plot(cols, row1_fit, linestyle=\"\", marker=\"o\", color=\"red\")\n",
    "\n",
    "row2_truth = [x.real if col in real_cols else x.imag for col, x in zip(cols, df_scaled_truth.iloc[11][cols])]\n",
    "row2_fit = [x.real if col in real_cols else x.imag for col, x in zip(cols, df_scaled_fit.iloc[11][cols])]\n",
    "ax2.plot(cols, row2_truth, color=\"black\")\n",
    "ax2.plot(cols, row2_fit, linestyle=\"\", marker=\"o\", color=\"red\")\n",
    "plt.xticks(rotation=90, fontsize=6)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "mass = df_array[\"all_effects\"][\"data\"][\"m_center\"]\n",
    "bin_width = df_array[\"all_effects\"][\"data\"][\"m_high\"] - df_array[\"all_effects\"][\"data\"][\"m_low\"]\n",
    "\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"truth_moments\"][\"H0(2,0,0,0)\"].map(lambda x: x.real), \n",
    "    linestyle=\"-\", color=colors[0], label=\"H0(2,0,0,0)\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"truth_moments\"][\"H1(0,0,0,0)\"].map(lambda x: x.real),\n",
    "    linestyle=\"--\", color=colors[1], label=\"H1(0,0,0,0)\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"truth_moments\"][\"H1(0,0,1,1)\"].map(lambda x: x.real),\n",
    "    linestyle=\":\", color=colors[2], label=\"H1(0,0,1,1)\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"truth_moments\"][\"H2(0,0,1,1)\"].map(lambda x: x.imag),\n",
    "    linestyle=\"-.\", color=colors[3], label=\"H2(0,0,1,1)\"\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"fit_moments\"][\"H0(2,0,0,0)\"].map(lambda x: x.real),\n",
    "    linestyle=\"\", color=colors[0], marker=\"s\", label=\"\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"fit_moments\"][\"H1(0,0,0,0)\"].map(lambda x: x.real),\n",
    "    linestyle=\"\", color=colors[1], marker=\"o\", label=\"\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"fit_moments\"][\"H1(0,0,1,1)\"].map(lambda x: x.real),\n",
    "    linestyle=\"\", color=colors[2], marker=\"P\", label=\"\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"fit_moments\"][\"H2(0,0,1,1)\"].map(lambda x: x.imag),\n",
    "    linestyle=\"\", color=colors[3], marker=\"X\", label=\"\"\n",
    ")\n",
    "plt.xlabel(r\"$\\omega\\pi^0$ inv. mass (GeV)\", loc=\"right\")\n",
    "plt.ylabel(f\"Moment / {bin_width.iloc[0]:.3f} GeV\", loc=\"top\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"truth_moments\"][\"H0(2,0,0,0)\"].map(lambda x: x.real),\n",
    "    linestyle=\"-\", color=colors[0], label=\"H0(2,0,0,0)\"\n",
    ")\n",
    "plt.plot(\n",
    "    mass, df_array[\"all_effects\"][\"fit_moments\"][\"H0(2,0,0,0)\"].map(lambda x: x.real),\n",
    "    linestyle=\"\", color=colors[0], marker=\"s\", label=\"\"\n",
    ")\n",
    "plt.xlabel(r\"$\\omega\\pi^0$ inv. mass (GeV)\", loc=\"right\")\n",
    "plt.ylabel(f\"Moment / {bin_width.iloc[0]:.3f} GeV\", loc=\"top\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"H0(2,0,0,0)\", \"H0(0,0,1,1)\", \"H1(2,0,0,0)\"]\n",
    "\n",
    "df_scaled_truth[\"H0(2,0,0,0)\"].map(lambda x: x.real).plot(linestyle=\"-\", color=\"black\")\n",
    "df_scaled_truth[\"H0(0,0,1,1)\"].map(lambda x: x.real).plot(linestyle=\"--\", color=\"blue\")\n",
    "df_scaled_truth[\"H1(2,0,0,0)\"].map(lambda x: x.real).plot(linestyle=\":\", color=\"orange\")\n",
    "df_scaled_truth[\"H2(0,0,1,1)\"].map(lambda x: x.imag).plot(linestyle=\"-.\", color=\"pink\")\n",
    "\n",
    "df_scaled_fit[\"H0(2,0,0,0)\"].map(lambda x: x.real).plot(linestyle=\"\", marker=\"s\", color=\"black\", label=\"\")\n",
    "df_scaled_fit[\"H0(0,0,1,1)\"].map(lambda x: x.real).plot(linestyle=\"\", marker=\"o\", color=\"blue\", label=\"\")\n",
    "df_scaled_fit[\"H1(2,0,0,0)\"].map(lambda x: x.real).plot(linestyle=\"\", marker=\"P\", color=\"orange\", label=\"\")\n",
    "df_scaled_fit[\"H2(0,0,1,1)\"].map(lambda x: x.imag).plot(linestyle=\"\", marker=\"X\", color=\"pink\", label=\"\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neutralb1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
