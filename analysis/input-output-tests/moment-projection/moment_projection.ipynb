{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Detector Effects Inducing Ambiguous $\\omega\\pi^0$ PWA Results?\n",
    "The results of the [detector effects](../detector_effects/detector_effects.ipynb) input-output study showed no clear indication of where the bin-to-bin inconsistencies are occurring in $\\omega\\pi^0$ partial wave analysis. No single detector effect appeared to be responsible for the problem, but it was clear that when using the *ideal* or \"thrown\" Monte Carlo data, there were no issues (aside from the expected fluctuations in negative reflectivity $\\varepsilon=-1$ waves). This suggests that the detector effects could be \"muddying\" the likelihood to such a degree that an unambiguous minimum is no longer obtainable. The best way to check for ambiguities is to project the partial wave results to **Moments**. \n",
    "\n",
    "In short, moments $H$ are the expansion coefficients of orthogonal basis functions \n",
    "$$\n",
    "    I^\\alpha(\\Omega,\\Omega_H) = \n",
    "        \\sum_{J_v=0}^2\n",
    "        \\sum_{\\Lambda=-2}^{2}\n",
    "        \\sum_{J=0}^{2*\\text{max}(J_i)}\n",
    "        \\sum_{M=-2*\\text{max}(m_i)}^{2*\\text{max}(m_i)}\n",
    "        \\frac{2J+1}{4\\pi} \\frac{2J_v+1}{4\\pi}\n",
    "        H^\\alpha(J_v,\\Lambda,J,M) \n",
    "        D_{M,\\Lambda}^{J\\ast}(\\Omega) D_{\\Lambda,0}^{J_v\\ast}(\\Omega_H)\n",
    "        \\,,\n",
    "$$\n",
    "here the Wigner D-functions $D_{m,n}^i(\\theta,\\phi,0)$, that describe our intensity. Though they lack the physical interpretation that partial waves provide, they are unique by construction i.e. do not suffer from ambiguous solutions. Moments can be written in terms of spin density matrix elements $\\rho_{i,j,m_i,m_j}$ as\n",
    "\\begin{align*}\n",
    "H^\\alpha(J_v,\\Lambda,J,M) = \n",
    "        &\\sum_{i=0}^{\\text{max}(J\\ell)_i}\n",
    "        \\sum_{j=0}^{\\text{max}(J\\ell)_j}\n",
    "        \\sum_{m_i=-J_i}^{J_i}\n",
    "        \\sum_{m_j=-J_j}^{J_j}\n",
    "        \\sum_{\\lambda=-1}^1\n",
    "        \\sum_{\\lambda'=-1}^1\n",
    "        \\frac{1}{2J_j+1} \\frac{1}{3}\n",
    "        \\\\\n",
    "        &\\times\n",
    "        \\braket{\\ell_i,0;1,\\lambda|J_i,\\lambda}\n",
    "        \\braket{\\ell_j,0;1,\\lambda'|J_j,\\lambda'}\n",
    "        \\\\\n",
    "        &\\times\n",
    "        \\braket{1,\\lambda;J_v,\\Lambda|1,\\lambda'}\n",
    "        \\braket{1,0;J_v,0|1,0}        \n",
    "        \\\\\n",
    "        &\\times \n",
    "        \\braket{J_i,m_i;J,M|J_j,m_j}\n",
    "        \\braket{J_i,\\lambda;J,\\Lambda|J_j,\\lambda'}\n",
    "        \\rho_{i,j,m_i,m_j}^\\alpha\n",
    "        \\Psi^i(w)\\Psi^{j\\ast}(w)\n",
    "    \\,,\n",
    "\\end{align*}\n",
    "which are composed of the PWA complex production coefficients $[c]$ in the reflectivity basis\n",
    "\\begin{align}    \n",
    "    \\rho^0_{i,j,m_i,m_j} &=\n",
    "    \\sum_\\varepsilon\n",
    "        [c^i]_{m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} +\n",
    "        (-1)^{m_i+m_j+\\ell_i+\\ell_j+J_i+J_j}\n",
    "        [c^i]_{-m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "    \\,,\n",
    "    \\\\          \n",
    "    \\rho^1_{i,j,m_i,m_j} &=\n",
    "    \\sum_\\varepsilon\n",
    "        \\varepsilon \\left(\n",
    "            (-1)^{1+m_i+\\ell_i+J_i}\n",
    "            [c^i]_{-m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} +\n",
    "            (-1)^{1+m_j+\\ell_j+J_j}\n",
    "            [c^i]_{m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "        \\right)\n",
    "    \\,,    \n",
    "    \\\\  \n",
    "    \\rho^2_{i,j,m_i,m_j} &= i\n",
    "    \\sum_\\varepsilon\n",
    "        \\varepsilon \\left(\n",
    "            (-1)^{m_i+\\ell_i+J_i}\n",
    "            [c^i]_{-m_i}^{\\varepsilon} [c^j]_{m_j}^{\\varepsilon\\ast} -\n",
    "            (-1)^{m_j+\\ell_j+J_j}\n",
    "            [c^i]_{m_i}^{\\varepsilon} [c^j]_{-m_j}^{\\varepsilon\\ast}\n",
    "        \\right)  \n",
    "    \\,.    \n",
    "\\end{align}\n",
    "For a more detailed explanation of these moments and how they can be obtained from the original partial wave description of the intensity, see [the linked note](https://halldweb.jlab.org/doc-private/DocDB/ShowDocument?docid=6715). When ambiguities are present this effectively means different combinations of partial wave values can reproduce the same intensity value, and thus the same moments. The core problem is one of an underdetermined system of equations, in which there are simply more free parameters (partial waves) than equations to constrain them. By projecting out the moments for two partial wave fit results, we can determine whether or not they are part of the same unique solution, and thus ambiguous.\n",
    "\n",
    "This study will have a very similar structure to the detector effects study, where a \"truth\" file for each detector effect that contains the true generated values is used as a comparison point for \"truth-initialized\" fits, or a fit whose parameters are initialized to the generated values. This study will project out the moment values from the truth and truth-initialized results in each detector effect situation. If the fit result agrees with the truth values, such as in the *ideal* case, then there is no concern, as its clear the fit is able to arrive at the true solution. If the partial wave fits do not agree, then we can expect the following two outcomes:\n",
    "\n",
    "1. **The projected moment values agree** and thus our each fit is one of potentially many ambiguous solutions. As stated before, differing partial wave results with identical moments means that we have found a case in which the same intensity value can be described by different sets of partial wave values.\n",
    "2. **The projected moment values disagree** and so we can conclude that our fit is diverging towards a uniquely different solution from the true one. \n",
    "\n",
    "Both situations are troubling. The only way to solve the ambiguities of case 1 would be to add constraints e.g. mass-dependent functions and/or removing waves. Case 2 does not rule out whether the true minima has ambiguities, and tells us that for some odd reason the fit diverges to a wrong result, despite being initialized right at the true set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Details\n",
    "At its core, this is an input-output study. The input is [GlueX $\\omega\\pi^0$ Monte Carlo](https://halldweb.jlab.org/wiki-private/index.php/Omega_Pi_Simulation_Samples_Version_3#Neutral_signal_versionsver3.1) signal (`ver3.1`) and phasespace (`ver03`) files, that are passed through a DSelector which turns on/off several detector effects. The output is the partial wave fits, whose values are initialized to the \"true\" values that generated the input, and the corresponding moments projected from those fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "We have the following effects available to study:\n",
    "1. **No effects (referred to as *thrown* or *ideal*):** This is the \"perfect detector\" scenario where all tracks are detected, reconstructed, and identified exactly\n",
    "2. **Acceptance Effects:** In this case the detector acceptance is applied, and so we only use tracks that are detected in the simulation. Note that the 4-momenta still match their generated values\n",
    "3. **Reconstruction (or *matched*):** Now we apply the reconstruction algorithm, and so our 4 momenta now have some resolution effects applied to them. This is called the *matched* case because we are still using perfect identification of the particles i.e. we know precisely which two photons were produced from the $\\pi^0$ in the $\\omega\\rightarrow \\pi^+\\pi^-\\pi^0$ decay, which otherwise could have been confused with the $\\pi^0$ in the $X\\rightarrow \\omega\\pi^0$ decay. We do this by \"matching\" the generated to the reconstructed tracks for the photons. In this way we use the reconstructed values but have no mis-identification possibilities\n",
    "   1. Note that we are only matching the $\\pi^0$'s, but in theory the $\\pi^+$ from the $\\omega$ could be confused with the recoil proton. This is highly unlikely to occur though, and so is of no concern.\n",
    "4. **All Effects:** This final step applies the detector effects, reconstruction algorithm, and allows for $\\pi^0$ combinatorics to occur, thus providing the closest approximation of real data as possible.\n",
    "\n",
    "In addition to the above, every one of these steps has an additional effect we can tack on: the out-of-time beam photons. When we use any \"no-accidental\" data, it means that our beam photon 4-momenta comes from the exact photon that generated the event. This is an idealized case, and we can instead simulate real data by having a set of out-of-time photons lie under the prompt in-time peak. To best determine what photon generated the event, we will then have to use the RF sideband subtraction method to remove these out-of-time photons, which is of course not perfect and can cause us to use the wrong photon. In short, any dataset labelled \"noaccidental\" uses the precise beam photon that generated the event, and datasets without this flag use RF sideband subtraction.\n",
    "\n",
    "All together we have 8 datasets: 4 simulated effects + with/without RF sideband subtraction for each.\n",
    "\n",
    "For any of these detector scenarios, the data is separated into bins of $\\omega\\pi^0$ mass 20 MeV wide, from 1.0 - 2.0 GeV. It uses the coherent peak energy range (8.2 - 8.8 GeV) and selects events within the the four moment transfer window $0.3 < -t < 0.5~GeV^2$. There is nothing particularly special about this window, but was simply chosen to limit the dataset file sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the truth information\n",
    "We'd like to compare how well our output aligns with our true generated values, but obtaining those true values is not so simple. As discussed, this input comes from a mass-dependent fit, causing 2 problems\n",
    "1. The overall scale of the production parameters are sensitive to the total intensity i.e. the number of events, but the input data is generated with a different number of events than the we originally fit to, and these events are further modified by detector effects\n",
    "2. The production parameters are constant for the entire mass range, and are modulated continuously by the Breit-Wigners. We want the values for individual mass bins.\n",
    "   \n",
    "We can alleviate both of these issues by performing a highly constrained fit in each mass bin using a `truth.cfg` file. This file fixes all the production coefficients to their generated values, but multiplies them all by an `intensity_scale` factor, which is left floating in the fit. This forces the fitter to keep the same interference behavior and allow the fit to adjust to any overall intensity, thus addressing Problem 1. The data is partitioned already into bins of 20 MeV, and so by running these fits with this config, the included Breit-Wigner functions will properly adjust the production coefficients for this narrow mass bin, handling Problem 2. This provides us with a set of properly scaled production coefficients, and therefore amplitudes, to compare with in each mass bin for any detector scenario. The moments can also be projected from these production coefficients, giving us a set of \"truth moments\" that we can compare our output \"fit moments\" to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "The output we want to analyze will be the results of our truth-initialized fits. The truth-initialized fits are nearly identical to the `truth.cfg` file, but here we are letting the production parameters (which are initialized to the true values) float and fixing the `intensity_scale` parameter to the value obtained by the truth fit for that bin. In other words, we are starting the fit off at the true set of production coefficients, and we use the result of the truth fit to properly scale those coefficients for the number of events in that bin. By all expectations, we should be starting the fit off at the likelihood minimum and don't expect these truth-initialized fits to diverge from where they start. We can of course project out the moment values of these truth-initialized fits and compare them with our true moment values. As discussed in the introduction, if the amplitude outputs are different than the inputs, then the moments can be used to check for the presence of underconstrained ambiguities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping for uncertainties\n",
    "When comparing amplitude values, we often bootstrap the data to get a better approximation of the statistical errors compared to the MINUIT estimated error. This procedure is important for amplitudes, but necessary for moments, as propagating the MINUIT uncertainties from the production coefficients all the way to the moments is complicated. We can avoid this difficult propagation by instead projecting out the moments from the bootstrapped fits, thus providing an approximation on the error for the moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Pre-Processing\n",
    "\n",
    "### Packages\n",
    "Lets start by loading in some packages and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some typical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "\n",
    "# load in default matplotlib style\n",
    "plt.style.use(\"/w/halld-scshelf2101/kscheuer/neutralb1/analysis/scripts/pwa_plotter.mplstyle\")\n",
    "\n",
    "# load useful paths and functions\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "PARENT_DIR = str(Path().resolve().parents[2])\n",
    "WORKING_DIR = f\"{PARENT_DIR}/analysis/input-output-tests/moment-projection/\"\n",
    "sys.path.insert(0, PARENT_DIR)\n",
    "import analysis.scripts.pwa_tools as pwa_tools\n",
    "\n",
    "# Load the environment variables. This contains needed setup_gluex.sh variables e.g. $ROOTSYS, $HALLD_HOME\n",
    "import subprocess\n",
    "command = f\"bash -l -c 'source {PARENT_DIR}/setup_gluex.sh && env'\"\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, executable='/bin/bash')\n",
    "output, _ = proc.communicate()\n",
    "# Parse the environment variables\n",
    "env_vars = {}\n",
    "for line in output.decode().splitlines():       \n",
    "    # the output contains a bunch of BASH_FUNCS that will ruin the environment variables. this avoids those issues\n",
    "    if len(line.split('=', 1)) != 2 or line.startswith(\"BASH_FUNC\") or line.startswith(\" \") or line.startswith(\"\\t\"):\n",
    "        continue    \n",
    "    key, value = line.split('=', 1)\n",
    "    env_vars[key] = value\n",
    "os.environ.update(env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also setup a few other useful constants as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_list = [\"thrown\", \"accept_noacc\", \"accept\", \"matched_noacc\", \"matched\", \"all_noacc\", \"all_effects\"]\n",
    "# when showing scenarios on same plots, we'll want a consistent color and marker scheme\n",
    "SCENARIO_COLORS = dict(zip(scenario_list, sns.color_palette(\"deep\", len(scenario_list))))\n",
    "SCENARIO_MARKERS = dict(zip(scenario_list, [\"o\", \"s\", \"D\", \"P\", \"X\", \"v\", \"^\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading fit results\n",
    "The following cell is in raw mode to avoid unnecessary execution, and is kept here as a reminder of what flags were used to submit the fits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%run -i $parent_dir/submission/batch_scripts/submit.py -w 1p 1m iso -o PARA_0 -m 1.00 2.00 0.02 -t 0.3 0.5 0.2 --data-version ver03.1 --data-option _mcthrown --phasespace-version ver03, --phasespace-option  -c 0.0 -g 1 A100 --time-limit 0:30:00 --truth-file truth_ver03.1.cfg --email kscheuer@jlab.org --email-type FAIL\n",
    "# replace \"mcthrown\" in --data_option for each case (_accept, _matched, \"\") + _noaccidental option for each\n",
    "# replace the phasespace-option for each detector case (_accept, _matched, \"\"). \n",
    "#     Remember the thrown case uses the gen phasepace \n",
    "#     The phasespace also has an implicit \"noaccidental\" flag built into it, and so can be used for both cases e.g. matched & matched_noaccidental\n",
    "# replace \"truth_ver03.1.cfg\" with \"truth__init_ver03.1.cfg\" to do the separate truth initialized fits\n",
    "# add a \"-b 100\" for 100 bootstrap fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now convert all the results into the needed csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tempfile\n",
    "def collect_csv_files(parent_path:str, scenario:str) -> None:\n",
    "    \"\"\"Aggregate all the different csv files for a given scenario\n",
    "\n",
    "    Makes a csv for the the truth, fit, and data files, and the moments of the truth and fit files.\n",
    "    Also make fit and moment csv files for the bootstrap fits.\n",
    "    parent_path must end at the truth subdirectory\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure the parent path is valid\n",
    "    parent_path += \"/\" if not parent_path.endswith(\"/\") else \"\"\n",
    "    parent_path = os.path.expanduser(parent_path) # expand the ~\n",
    "    if not parent_path.endswith(\"truth/\"):\n",
    "        raise ValueError(\"parent_path must end at the truth subdirectory\")\n",
    "    \n",
    "    # path to the conversion script and common output directory\n",
    "    script = f\"{PARENT_DIR}/analysis/scripts/collect_csv.py\"\n",
    "    output_dir = f\"{WORKING_DIR}{scenario}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        raise FileNotFoundError(f\"Directory {output_dir} does not exist\")\n",
    "\n",
    "    # input path for each file\n",
    "    input_files = [\n",
    "        \"best.csv\", # truth\n",
    "        \"init/best.csv\", # fits\n",
    "        \"data.csv\", # data\n",
    "        \"best_moments_table.csv\", # production coefficient table\n",
    "        \"best_moments.csv\", # truth moments\n",
    "        \"init/best_moments.csv\", # fit moments        \n",
    "        \"init/bootstrap/bootstrap.csv\", # bootstrap fits\n",
    "        \"init/bootstrap/bootstrap_moments.csv\", # bootstrap moments        \n",
    "    ]\n",
    "\n",
    "    output_names = [ # in same order as input_files\n",
    "        \"truth.csv\",\n",
    "        \"fit.csv\",\n",
    "        \"data.csv\",\n",
    "        \"table.csv\",\n",
    "        \"truth_moments.csv\",        \n",
    "        \"fit_moments.csv\",        \n",
    "        \"bootstrap.csv\",\n",
    "        \"bootstrap_moments.csv\",\n",
    "    ]\n",
    "    # create csvs if they don't exist\n",
    "    for f, name in zip(input_files, output_names):        \n",
    "        if not os.path.exists(f\"{output_dir}/{name}\"):    \n",
    "            full_input = glob.glob(f\"{parent_path}{f}\") # expand the wildcards\n",
    "            # use a tempfile to store all the files to not overload the subprocess command\n",
    "            with tempfile.NamedTemporaryFile(delete=False, mode='w') as temp_file:\n",
    "                temp_file.write(\"\\n\".join(full_input))\n",
    "                temp_file_path = temp_file.name                       \n",
    "            \n",
    "            subprocess.run([\"python\", script, \"-i\", temp_file_path, \"-o\", f\"{output_dir}/{name}\"])            \n",
    "        else:\n",
    "            print(f\"{output_dir}/{name} already exists, skipping\")\n",
    "\n",
    "    return\n",
    "\n",
    "common_path = (\n",
    "    \"~/volatile/ampToolsFits/omegapi/allPeriods/PARA_0/ver03.1_mcSCENARIO/\"\n",
    "    \"ver03OPTION/1m_1p_iso/recoil-pi-mass_0.0/t_0.30-0.50/*/truth/\"\n",
    ")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"thrown\").replace(\"OPTION\", \"\"), \"thrown\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"accept_noaccidental\").replace(\"OPTION\", \"_accept\"), \"accept_noaccidental\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"accept\").replace(\"OPTION\", \"_accept\"), \"accept\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"matched_noaccidental\").replace(\"OPTION\", \"_matched\"), \"matched_noaccidental\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"matched\").replace(\"OPTION\", \"_matched\"), \"matched\")\n",
    "# collect_csv_files(common_path.replace(\"SCENARIO\", \"noaccidental\").replace(\"OPTION\", \"\"), \"all_noaccidental\")\n",
    "collect_csv_files(common_path.replace(\"SCENARIO\", \"\").replace(\"OPTION\", \"\"), \"all_effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then load all of these into pandas dataframes. Pandas will interpret the moment csv's to be strings, so we need to ensure they're interpreted as complex values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(subdir:str) -> Dict[str, pd.DataFrame]:\n",
    "    if not subdir.endswith(\"/\"):\n",
    "        subdir += \"/\"\n",
    "\n",
    "    # load dataframes. Assumes files all in same format for any subdir\n",
    "    truth = pd.read_csv(f\"{subdir}truth.csv\")\n",
    "    truth_moments = pd.read_csv(f\"{subdir}truth_moments.csv\")\n",
    "    fit = pd.read_csv(f\"{subdir}fit.csv\")\n",
    "    fit_moments = pd.read_csv(f\"{subdir}fit_moments.csv\")\n",
    "    bootstrap = pd.read_csv(f\"{subdir}bootstrap.csv\")\n",
    "    bootstrap_moments = pd.read_csv(f\"{subdir}bootstrap_moments.csv\")\n",
    "    data = pd.read_csv(f\"{subdir}data.csv\")\n",
    "    table = pd.read_csv(f\"{subdir}table.csv\")\n",
    "\n",
    "    # make sure moment dataframes register as complex values (except for the file column)\n",
    "    truth_moments.loc[:, truth_moments.columns != \"file\"] = truth_moments.loc[:, truth_moments.columns != \"file\"].astype(complex)\n",
    "    fit_moments.loc[:, fit_moments.columns != \"file\"] = fit_moments.loc[:, fit_moments.columns != \"file\"].astype(complex)\n",
    "    bootstrap_moments.loc[:, bootstrap_moments.columns != \"file\"] = bootstrap_moments.loc[:, bootstrap_moments.columns != \"file\"].astype(complex)    \n",
    "\n",
    "    df_dict = {\n",
    "        \"truth\" : truth,\n",
    "        \"truth_moments\" : truth_moments,\n",
    "        \"fit\" : fit,\n",
    "        \"fit_moments\" : fit_moments,\n",
    "        \"bootstrap\" : bootstrap,\n",
    "        \"bootstrap_moments\" : bootstrap_moments,\n",
    "        \"data\" : data,\n",
    "        \"table\": table\n",
    "    }\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "# Load all the dataframes\n",
    "thrown_dfs = load_dataframes(f\"{WORKING_DIR}thrown/\")\n",
    "accept_noacc_dfs = load_dataframes(f\"{WORKING_DIR}accept_noaccidental/\")\n",
    "accept_dfs = load_dataframes(f\"{WORKING_DIR}accept/\")\n",
    "matched_noacc_dfs = load_dataframes(f\"{WORKING_DIR}matched_noaccidental/\")\n",
    "matched_dfs = load_dataframes(f\"{WORKING_DIR}matched/\")\n",
    "# all_noacc_dfs = load_dataframes(f\"{WORKING_DIR}all_noaccidental/\")\n",
    "all_effect_dfs = load_dataframes(f\"{WORKING_DIR}all_effects/\")\n",
    "\n",
    "all_dfs = {\n",
    "    \"thrown\": thrown_dfs,\n",
    "    \"accept_noacc\": accept_noacc_dfs,\n",
    "    \"accept\": accept_dfs,\n",
    "    \"matched_noacc\": matched_noacc_dfs,\n",
    "    \"matched\": matched_dfs,\n",
    "    # \"all_noacc\": all_noacc_dfs,\n",
    "    \"all_effects\": all_effect_dfs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes are indexed by the file name, which shares a common mass bin with other dataframes like \"path/mass_x.x-y.y/subdir/\". We'll add a new column \"mass\" to the dataframes that contain this mass bin, which will make comparisons across dataframes easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mass(file_name):\n",
    "    match = re.search(r\"/mass_([\\d.]+-[\\d.]+)\", file_name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise ValueError(\"Mass not found in file name.\")\n",
    "    \n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    for name, df in df_dict.items():\n",
    "        if name == \"table\":\n",
    "            continue\n",
    "        if \"mass\" not in df.columns:\n",
    "            df[\"mass\"] = df[\"file\"].apply(extract_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Before we get into any real analysis, we'll want to run some common checks on our moments to ensure that they're projection from amplitudes $\\rightarrow$ moments ran as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real & Imaginary Parts\n",
    "First let's check that the imaginary(real) parts of the $H^0,H^1$($H^2$) moments are zero as we expect. Lets plot these components as a function of mass for each file's truth and fit moments. We'll plot them all together, even though the plot can get crowded, just to check that they all within some floating point error tolerance of zero. We can also use this chance to check that our truth and fit results have the same moment set as well i.e. number of rows and columns in the csv's match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in all_dfs.items():\n",
    "    if val[\"truth_moments\"].shape[1] != val[\"fit_moments\"].shape[1] or val[\"truth_moments\"].shape[1] != val[\"bootstrap_moments\"].shape[1]:\n",
    "        raise ValueError(f\"Number of moments do not match for {key}\")\n",
    "    if val[\"truth_moments\"].shape[0] != val[\"fit_moments\"].shape[0]:\n",
    "        raise ValueError(f\"Number of files does not match for {key}\")\n",
    "\n",
    "    h0_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H0\")]\n",
    "    h1_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H1\")]\n",
    "    h2_columns = [col for col in val[\"truth_moments\"].columns if col.startswith(\"H2\")]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    mass_bins = val[\"data\"][\"m_center\"]\n",
    "    bin_width = (val[\"data\"][\"m_high\"] - val[\"data\"][\"m_low\"])[0]\n",
    "\n",
    "    # plot truth moments with lines and fit moments as points\n",
    "    val[\"truth_moments\"][h0_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"-\", color=\"blue\", alpha=0.8, legend=False)\n",
    "    val[\"fit_moments\"][h0_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"\", marker=\"o\", color=\"blue\", alpha=0.4, legend=False)\n",
    "    val[\"truth_moments\"][h1_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"--\", alpha=0.8, color=\"orange\", legend=False)\n",
    "    val[\"fit_moments\"][h1_columns].map(lambda x: x.imag).plot(ax=ax, linestyle=\"\", marker=\"s\", color=\"orange\", alpha=0.4, legend=False)\n",
    "    val[\"truth_moments\"][h2_columns].map(lambda x: x.real).plot(ax=ax, linestyle=\":\", alpha=0.8, color=\"green\", legend=False)\n",
    "    val[\"fit_moments\"][h2_columns].map(lambda x: x.real).plot(ax=ax, linestyle=\"\", marker=\"D\", color=\"green\", alpha=0.4, legend=False)\n",
    "\n",
    "    # create some dummy lines for a nice legend\n",
    "    h0_dummy = mpl.lines.Line2D([0], [0], linestyle=\"-\", color=\"blue\", label=r\"$\\Im(H^0)$\")\n",
    "    h1_dummy = mpl.lines.Line2D([0], [0], linestyle=\"--\", color=\"orange\", label=r\"$\\Im(H^1)$\")\n",
    "    h2_dummy = mpl.lines.Line2D([0], [0], linestyle=\":\", color=\"green\", label=r\"$\\Re(H^2)$\")\n",
    "\n",
    "    ax.legend(handles=[h0_dummy, h1_dummy, h2_dummy])\n",
    "\n",
    "    print(f\"Plotting {key}\")\n",
    "    ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "    ax.set_ylabel(f\"Value / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having verified this, we will make the $H^0, H^1, (H^2)$ dataframes be purely real (imaginary) to make them much easier to work with in the rest of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario, df_dict in all_dfs.items():\n",
    "    for name, df in df_dict.items():\n",
    "        if \"moment\" not in name:\n",
    "            continue\n",
    "        h0_cols = [col for col in df.columns if col.startswith(\"H0\")]\n",
    "        h1_cols = [col for col in df.columns if col.startswith(\"H1\")]\n",
    "        h2_cols = [col for col in df.columns if col.startswith(\"H2\")]\n",
    "\n",
    "        # if we've already applied the transformation, skip\n",
    "        if all(df[h0_cols[0]].apply(lambda x: isinstance(x, (int, float)))):\n",
    "            continue\n",
    "\n",
    "        df[h0_cols] = df[h0_cols].map(lambda x: x.real)\n",
    "        df[h1_cols] = df[h1_cols].map(lambda x: x.real)\n",
    "        df[h2_cols] = df[h2_cols].map(lambda x: x.real)\n",
    "\n",
    "        # explicitly overwrite the dataframe\n",
    "        df_dict[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding non-zero moments\n",
    "For our analysis we're only interested in those moments who are non-zero, so we'll use the truth moments to determine this. We'll also run a check to make sure the fit projected moments have the same set of non-zero moments as the truth moments for that scenario, and the truth moments of all other scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-10\n",
    "\n",
    "nonzero_moments = []\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    # get the nonzero columns for each scenario\n",
    "    moment_columns = [col for col in df_dict[\"truth_moments\"] if col.startswith(\"H\")]\n",
    "    scenario_nonzero_moments = [col for col in moment_columns if (df_dict[\"truth_moments\"][col].abs() > tolerance).any()]\n",
    "\n",
    "    # check if the nonzero columns are the same for each scenario\n",
    "    if not nonzero_moments:\n",
    "        nonzero_moments = scenario_nonzero_moments\n",
    "    else:\n",
    "        if set(nonzero_moments) != set(scenario_nonzero_moments):\n",
    "            print(f\"Scenario {scenario} has different nonzero moments\")\n",
    "            print(set(nonzero_moments) - set(scenario_nonzero_moments))\n",
    "\n",
    "    # check if the nonzero columns are the same between truth and fit\n",
    "    fit_cols = [col for col in moment_columns if (df_dict[\"fit_moments\"][col].abs() > tolerance).any()]\n",
    "    if set(scenario_nonzero_moments) != set(fit_cols):\n",
    "        print(f\"Scenario {scenario} has different nonzero moments between truth and fit\")        \n",
    "        print(set(nonzero_moments) - set(fit_cols))\n",
    "        \n",
    "print(f\"Reducing moments for all dataframes to {len(nonzero_moments)} nonzero moment columns\")\n",
    "for col in nonzero_moments:\n",
    "    print(col)\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    for key in [\"truth_moments\", \"fit_moments\", \"bootstrap_moments\"]:\n",
    "        non_moment_columns = [col for col in df_dict[key].columns if not col.startswith(\"H\")]        \n",
    "        df_dict[key] = df_dict[key][non_moment_columns + nonzero_moments]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $H^0(0,0,0,0)$ =? Generated Events\n",
    "The next check we can perform is to see how close the $H^0(0,0,0,0)$ moment is to the number of generated events. If all factors are handled appropriately these two should match. Even if they don't, we will normalize them soon. Below we'll plot the truth and fit $H^0(0,0,0,0)$ moments with the \\# of generated events, and what scale factor these are off by (averaged over the mass bins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario, df_dict in all_dfs.items():\n",
    "    h0_truth = df_dict[\"truth_moments\"][\"H0(0,0,0,0)\"]\n",
    "    h0_fit = df_dict[\"fit_moments\"][\"H0(0,0,0,0)\"]\n",
    "    generated_events = df_dict[\"truth\"][\"generated_events\"]\n",
    "\n",
    "    scale_truth = (h0_truth / generated_events).mean()\n",
    "    scale_fit = (h0_fit / generated_events).mean()    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    mass_bins = df_dict[\"data\"][\"m_center\"]\n",
    "    bin_width = (df_dict[\"data\"][\"m_high\"] - df_dict[\"data\"][\"m_low\"])[0]\n",
    "\n",
    "    ax.bar(mass_bins, generated_events, width=bin_width, color=\"gray\", alpha=0.5, label=\"Generated Events\")    \n",
    "    ax.plot(mass_bins, h0_truth, color=\"black\", marker=\"\", linestyle=\"-\", label=rf\"$H^0(0,0,0,0)_{{\\text{{truth}}}} (x{scale_truth:.2f})$\")\n",
    "    ax.plot(mass_bins, h0_fit, color=\"blue\", marker=\"o\", linestyle=\"\", label=rf\"$H^0(0,0,0,0)_{{\\text{{fit}}}} (x{scale_fit:.2f})$\")\n",
    "\n",
    "    print(f\"Plotting {scenario}\")\n",
    "    ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "    ax.set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")    \n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "We will use normalized moments\n",
    "$$\n",
    "\\hat{H}^\\alpha(J_v,\\Lambda,J,M) = \\frac{H^\\alpha(J_v,\\Lambda,J,M)}{H^0(0,0,0,0)}.\n",
    "$$ \n",
    "for the rest of our analysis, as it will account for these common scale factors and differences in the number of events for each scenario. Note:\n",
    "* We make sure to include the propagation of uncertainty when doing this normalization\n",
    "* The $H^2$ columns are stored as float values, but in reality are `complex(0, a)`. Luckily the normalization and uncertainty propagation is simple here, but in the future we may want to store them as complex to not miscalculate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_moments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # avoid normalizing already normalized data   \n",
    "    if (df[\"H0(0,0,0,0)\"] == 1).all():             \n",
    "        return df\n",
    "    \n",
    "    moment_cols = [col for col in df.columns if col.startswith(\"H\") and col != \"H0(0,0,0,0)\"]\n",
    "    for col in moment_cols:           \n",
    "        df[col] = df[col].div(df[\"H0(0,0,0,0)\"])\n",
    "    df[\"H0(0,0,0,0)\"] = df[\"H0(0,0,0,0)\"].div(df[\"H0(0,0,0,0)\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    for key, df in df_dict.items():\n",
    "        if \"moment\" not in key:\n",
    "            continue        \n",
    "        df_dict[key] = normalize_moments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting errors from the bootstrap distributions\n",
    "The bootstrap values provide a better approximation of the error for the amplitudes. In the case of the moments, we cannot easily propagate the MINUIT production coefficient errors to the moments, and so the bootstrap provides the only error. Before using them, we want to make sure they're roughly gaussian distributed by using a [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) and ensuring that its resultant p-values are greater than 0.05. \n",
    "\n",
    "Distributions that do not pass this tests are plotted on a probability plot, grouped by mass bin and scenario, and saved to a pdf. These probability plots, made using `scipy.stats.probplot`, are the same as Q-Q plots [(despite what the documentation implies)](https://stackoverflow.com/questions/48108582/how-to-interpret-scipy-stats-probplot-results). Deviations from the best fit line indicate that the bootstrap distribution is not similar to a normal distribution, and we should be wary of using its standard deviation as an estimation of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "def shapiro_test(grouped_df:pd.DataFrame, columns:list, pdf: PdfPages) -> None:\n",
    "    # test the normality of the bootstrap data, grouped by file name\n",
    "\n",
    "    # use dict of dicts to store { col : {scenario : p-value} } for distributions that are not normal\n",
    "    non_normal_dict = {}\n",
    "    for col in columns:     \n",
    "        for scenario in grouped_df['scenario'].unique():   \n",
    "            stat, p_value = scipy.stats.shapiro(grouped_df[grouped_df['scenario'] == scenario][col])        \n",
    "            if p_value < 0.05:\n",
    "                non_normal_dict.setdefault(col, {})[scenario] = p_value\n",
    "\n",
    "    # Plot the non-normal columns\n",
    "    if non_normal_dict:\n",
    "        # setup the subplots to be a square grid\n",
    "        num_plots = len(non_normal_dict)\n",
    "        num_cols = math.ceil(math.sqrt(num_plots))\n",
    "        num_rows = math.ceil(num_plots / num_cols)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))\n",
    "        axes = axes.flatten() if num_plots > 1 else [axes]\n",
    "        \n",
    "        # plot all non-normal scenarios on the same plot\n",
    "        for ax, (col, scenario_dict) in zip(axes, non_normal_dict.items()):\n",
    "            for scenario, p_value in scenario_dict.items():\n",
    "                scipy.stats.probplot(grouped_df[grouped_df['scenario'] == scenario][col], dist=\"norm\", plot=ax)\n",
    "\n",
    "                ax.set_title(f\"{col}\")\n",
    "                    \n",
    "                best_fit_line = ax.get_lines()[-1] # always plotted last\n",
    "                data_markers = ax.get_lines()[-2]\n",
    "                \n",
    "                # change markers and best fit line to a color for that scenario\n",
    "                best_fit_line.set_color(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_markerfacecolor(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_markeredgecolor(SCENARIO_COLORS[scenario])\n",
    "                data_markers.set_marker(SCENARIO_MARKERS[scenario])\n",
    "                data_markers.set_label(f\"{scenario}:{p_value:.1e}\")\n",
    "                data_markers.set_alpha(0.7)\n",
    "                ax.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "        # create a figure legend explaining the scenario colors\n",
    "        handles = [\n",
    "            mpl.patches.Patch(color=color, label=scenario) \n",
    "            for scenario, color in SCENARIO_COLORS.items()\n",
    "        ]\n",
    "        fig.legend(handles=handles, loc='upper right', title='Scenarios', ncol=3)\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for ax in axes[num_plots:]:\n",
    "            ax.set_visible(False)\n",
    "        \n",
    "        fig.suptitle(f\"mass bin = {grouped_df.name}\", fontsize=20)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # give some more room for the title at the top\n",
    "        pdf.savefig(fig)\n",
    "        plt.close()\n",
    "\n",
    "# setup dataframes to hold all the bootstrap data, with their scenarios labelled\n",
    "all_bootstrap_moment = pd.DataFrame()    \n",
    "all_bootstrap = pd.DataFrame()   \n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    # use dataframe copies to avoid modifying the original dataframes\n",
    "    copy_df = df_dict[\"bootstrap\"].copy()\n",
    "    copy_moment_df = df_dict[\"bootstrap_moments\"].copy()\n",
    "    copy_moment_df[\"scenario\"] = scenario        \n",
    "    copy_df[\"scenario\"] = scenario\n",
    "\n",
    "    all_bootstrap_moment = pd.concat([all_bootstrap_moment, copy_moment_df])\n",
    "    all_bootstrap = pd.concat([all_bootstrap, copy_df])\n",
    "\n",
    "del copy_moment_df\n",
    "del copy_df\n",
    "\n",
    "moment_columns = [col for col in all_bootstrap_moment.columns if col.startswith(\"H\")]\n",
    "grouped_bootstrap_moments_df = all_bootstrap_moment.groupby(\"mass\")\n",
    "\n",
    "# apply the shapiro test to the grouped dataframe, and save all figs into a pdf\n",
    "# we only apply this to the moments, since the amplitudes and phases are bound to be >0, making this \n",
    "# test not very useful for them. We'll need to apply a different test for those in the future.\n",
    "with PdfPages(\"shapiro_bootstrap_moments.pdf\") as pdf:        \n",
    "    grouped_bootstrap_moments_df.apply(\n",
    "        shapiro_test, \n",
    "        columns=moment_columns, pdf=pdf, \n",
    "        include_groups=False\n",
    "    )\n",
    "    print(f\"Shapiro test results for bootstrapped moments saved to: {os.path.join(WORKING_DIR, 'shapiro_bootstrap_moments.pdf')}\")\n",
    "\n",
    "# Explicitly delete the grouped dataframe for memory\n",
    "del grouped_bootstrap_moments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked the bootstrap distributions are gaussian-like enough to use, we can use their standard deviations as the errors for our moments. Lets use the grouped bootstrap moment dataframe we just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"scenario\" and \"mass\" and calculate the standard deviation for each grouping\n",
    "# we only want to use the bootstrap errors for the amplitude values, because the phase standard deviations aren't properly done\n",
    "amp_columns = [item for sublist in pwa_tools.get_coherent_sums(all_bootstrap).values() for item in sublist]\n",
    "amp_columns += [\"mass\", \"scenario\"]\n",
    "std_dev = all_bootstrap[amp_columns].groupby([\"mass\", \"scenario\"]).std().reset_index()\n",
    "std_dev_moments = all_bootstrap_moment.drop(columns=\"file\").groupby([\"mass\", \"scenario\"]).std().reset_index()\n",
    "\n",
    "# add the standard deviations as errors to the fit dataframes\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    scenario_std_dev = std_dev[std_dev[\"scenario\"] == scenario].drop(columns=\"scenario\")\n",
    "    scenario_std_dev_moments = std_dev_moments[std_dev_moments[\"scenario\"] == scenario].drop(columns=\"scenario\")\n",
    "\n",
    "    # drop the old error columns for the amplitude fits\n",
    "    amp_columns_errs = [f\"{col}_err\" for col in amp_columns if col not in [\"mass\", \"scenario\"]]\n",
    "    df_dict[\"fit\"] = df_dict[\"fit\"].loc[:, ~df_dict[\"fit\"].columns.isin([c for c in amp_columns_errs if c in df_dict[\"fit\"].columns])]\n",
    "\n",
    "    # merge the standard deviation dataframes with the fit dataframes, this ensures the masses match correctly\n",
    "    df_dict[\"fit\"] = df_dict[\"fit\"].merge(\n",
    "        scenario_std_dev,\n",
    "        on=[\"mass\"],\n",
    "        suffixes=(\"\", \"_err\"),\n",
    "    )\n",
    "    df_dict[\"fit_moments\"] = df_dict[\"fit_moments\"].merge(\n",
    "        scenario_std_dev_moments,\n",
    "        on=[\"mass\"],\n",
    "        suffixes=(\"\", \"_err\"),\n",
    "    )\n",
    "\n",
    "# delete these now that we're done with them\n",
    "del std_dev\n",
    "del std_dev_moments\n",
    "del scenario_std_dev\n",
    "del scenario_std_dev_moments\n",
    "del all_bootstrap\n",
    "del all_bootstrap_moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truth Moments Vs Detector Effects\n",
    "Aside from an overall scale factor, we don't expect the truth moments to change for each detector effect. We've accounted for this scale factor already by using the normalized moments, so we can plot them to check that they're shape is identical across the mass bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check if our truth moments are identical for each detector effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this cell in line with the others styles. No need for .real, .imag, etc. Use df_dict in for loop\n",
    "cols = all_dfs[\"thrown\"][\"truth_moments\"].columns\n",
    "nonzero_h0 = [col for col in cols if col.startswith(\"H0\")]\n",
    "nonzero_h1 = [col for col in cols if col.startswith(\"H1\")]\n",
    "nonzero_h2 = [col for col in cols if col.startswith(\"H2\")]\n",
    "\n",
    "# setup a dataframe to store the truth moments for each scenario\n",
    "truth_df = pd.DataFrame()\n",
    "\n",
    "for scenario, df in all_dfs.items():\n",
    "    # get the nonzero real(imaginary) parts of the H0,H1,(H2) moments\n",
    "    h0_truth = df[\"truth_moments\"][nonzero_h0].map(lambda x: x.real)\n",
    "    h1_truth = df[\"truth_moments\"][nonzero_h1].map(lambda x: x.real)\n",
    "    h2_truth = df[\"truth_moments\"][nonzero_h2].map(lambda x: x.imag)\n",
    "\n",
    "    # add those truth moments to the dataframe with the mass and scenario\n",
    "    df = pd.concat([df[\"data\"][\"m_center\"], h0_truth, h1_truth, h2_truth], axis=1)\n",
    "    df.rename(columns={\"m_center\": \"mass\"}, inplace=True)\n",
    "    df[\"scenario\"] = scenario\n",
    "    truth_df = pd.concat([truth_df, df])\n",
    "\n",
    "# plot the relative truth moments for each scenario\n",
    "truth_df_melted = truth_df.melt(id_vars=[\"mass\", \"scenario\"], var_name=\"moment\", value_name=\"truth\")\n",
    "grid = sns.relplot(\n",
    "    data=truth_df_melted, x=\"mass\", y=\"truth\", col=\"moment\", \n",
    "    col_wrap=10, kind=\"line\", hue=\"scenario\", style=\"scenario\", facet_kws={'sharey': False}\n",
    ")\n",
    "\n",
    "for ax in grid.axes.flat:\n",
    "    # remove the \"moment = \" from the title\n",
    "    subplot_title = ax.get_title()\n",
    "    ax.set_title(subplot_title.replace(\"moment = \", \"\"))\n",
    "    # set the alpha value of each line\n",
    "    for line in ax.get_lines():\n",
    "        scenario = line.get_label()\n",
    "        if scenario == \"thrown\":\n",
    "            line.set_alpha(1.0)\n",
    "        else:\n",
    "            line.set_alpha(0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "With our data verified and processed, lets move onto some analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Amplitude and Moment Plots\n",
    "We'll start off by making some standard plots, such as the moments and amplitudes as a function of mass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust plot ranges to be maximum between errors and truth values\n",
    "# plot the fit and truth moments as a function of mass, for each scenario\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    truth_moments = df_dict[\"truth_moments\"]\n",
    "    fit_moments = df_dict[\"fit_moments\"]\n",
    "\n",
    "    moment_cols = [c for c in truth_moments.columns if c.startswith(\"H\")]\n",
    "    num_plots = len(moment_cols)\n",
    "    num_cols = math.ceil(math.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        num_rows, num_cols, \n",
    "        figsize=(num_cols * 5, num_rows * 5),\n",
    "        sharex=True\n",
    "    )\n",
    "    axes = axes.flatten() if num_plots > 1 else [axes]\n",
    "\n",
    "    for i, moment in enumerate(moment_columns):\n",
    "        ax = axes[i]\n",
    "        mass = df_dict[\"data\"][\"m_center\"]\n",
    "        bin_width = (df_dict[\"data\"][\"m_high\"] - df_dict[\"data\"][\"m_low\"]).iloc[0]\n",
    "\n",
    "        fit_values = fit_moments[moment]\n",
    "        fit_errors = fit_moments[f\"{moment}_err\"]\n",
    "        \n",
    "        ax.errorbar(\n",
    "            x=mass, y=fit_values, yerr=fit_errors,\n",
    "            color=SCENARIO_COLORS[scenario], marker=SCENARIO_MARKERS[scenario],\n",
    "            linestyle=\"\", clip_on=True\n",
    "        )\n",
    "        ax.plot(\n",
    "            mass, truth_moments[moment], \n",
    "            color=SCENARIO_COLORS[scenario], marker=\"\", linestyle=\"-\"\n",
    "        )\n",
    "        ax.set_title(moment)\n",
    "\n",
    "        # set y-axis limits to focus on data\n",
    "        y_min = min(fit_values)\n",
    "        y_max = max(fit_values)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        if i >= num_plots - num_cols: # set xlabel for last plot of columns\n",
    "            ax.set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "        if i % num_cols == 0: # set ylabel for first column\n",
    "            ax.set_ylabel(f\"Value / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_plots:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    fig.suptitle(f\"Fit and Truth Moments for Scenario: {scenario}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make some plots of the individual amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario, df_dict in all_dfs.items():\n",
    "    plotter = pwa_tools.Plotter(df_dict[\"fit\"], df_dict[\"data\"], truth_df=df_dict[\"truth\"])\n",
    "    print(f\"Creating plots for the scenario: {scenario}\")    \n",
    "    plotter.intensities()\n",
    "    plotter.intensities(sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment $\\chi^2$ / ndf and Mean Squared Error (MSE)\n",
    "As we discussed in the introduction, if the moments projected from the fit results are close to the true moment values, then we can say with some certainty that our amplitude fit results are part of an ambiguous set of continuous solutions that include the true amplitude values. So how do we quantify \"close\"? Well the most straightforward way is a reduced chi-squared\n",
    "$$\n",
    "\\frac{\\chi^2}{\\text{ndf}} = \\frac{1}{\\text{ndf}} \\sum_{\\alpha,J_v,\\Lambda,J,M} \\left(\n",
    "    \\frac{\n",
    "        \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{truth}} - \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{fit}}\n",
    "    }{\n",
    "        \\sigma_{\\hat{H}}^{\\text{fit}}\n",
    "    }\n",
    "\\right)^2\\,,\n",
    "$$\n",
    "for some number of degrees of freedom $\\text{ndf}$. This statistic is weighted by the errors $\\sigma_{\\hat{H}}^{\\text{fit}}$ , which we obtained via the bootstrap distributions. We can separate this into 2 broad categories\n",
    "1. $\\chi^2/\\text{ndf} >> 1$: The fit projected moment set is different from the truth values, and thus the amplitude fits are a unique solution that is different from the true amplitudes\n",
    "2. $\\chi^2/\\text{ndf} \\approx 1$: The difference between the true and fit projected moments are within the variance, and so the its likely that the amplitude fits and true amplitudes are both equally valid descriptions of the data. If the fit and true amplitudes differ, then this is a case of continuous ambiguities.\n",
    "\n",
    "(Potential discussion of MSE here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_columns = [col for col in all_dfs[\"thrown\"][\"truth_moments\"].columns if col.startswith(\"H\") and not col.endswith(\"_err\")]\n",
    "\n",
    "# create total dataframes for every scenario\n",
    "residual_df = pd.DataFrame()\n",
    "weighted_residual_df = pd.DataFrame()\n",
    "mse_df = pd.DataFrame()\n",
    "reduced_chi2_df = pd.DataFrame()\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    # calculate the residual for each moment\n",
    "    scenario_residual_df = df_dict[\"truth_moments\"][moment_columns].sub(df_dict[\"fit_moments\"][moment_columns])\n",
    "    \n",
    "    # calculated weighted residuals with the proper error\n",
    "    scenario_weighted_df = scenario_residual_df.copy()\n",
    "    for col in scenario_weighted_df.columns:\n",
    "        scenario_weighted_df[col] = scenario_weighted_df[col].div(df_dict[\"fit_moments\"][f\"{col}_err\"])\n",
    "\n",
    "    # calculate the mean squared error and chi2 for the scenario\n",
    "    scenario_mse_df = (scenario_residual_df ** 2).sum(axis=1) / scenario_residual_df.count(axis=1)\n",
    "    scenario_chi2 = (scenario_weighted_df ** 2).sum(axis=1) / (scenario_weighted_df.count(axis=1) - 1)\n",
    "\n",
    "    # add the scenario and mass labels to the dataframes\n",
    "    scenario_residual_df[\"scenario\"] = scenario\n",
    "    scenario_residual_df[\"mass\"] = df_dict[\"truth_moments\"][\"mass\"]\n",
    "    scenario_weighted_df[\"scenario\"] = scenario\n",
    "    scenario_weighted_df[\"mass\"] = df_dict[\"truth_moments\"][\"mass\"]\n",
    "\n",
    "    scenario_mse_df = pd.DataFrame(\n",
    "        {\n",
    "            \"scenario\": [scenario] * len(df_dict[\"truth_moments\"]),\n",
    "            \"mass\": df_dict[\"truth_moments\"][\"mass\"],\n",
    "            \"mse\": scenario_mse_df\n",
    "        }\n",
    "    )\n",
    "    scenario_chi2 = pd.DataFrame(\n",
    "        {\n",
    "            \"scenario\": [scenario] * len(df_dict[\"truth_moments\"]),\n",
    "            \"mass\": df_dict[\"truth_moments\"][\"mass\"],\n",
    "            \"chi2\": scenario_chi2\n",
    "        }\n",
    "    )\n",
    "    residual_df = pd.concat([residual_df, scenario_residual_df])\n",
    "    weighted_residual_df = pd.concat([weighted_residual_df, scenario_weighted_df])\n",
    "    mse_df = pd.concat([mse_df, scenario_mse_df])\n",
    "    reduced_chi2_df = pd.concat([reduced_chi2_df, scenario_chi2])\n",
    "\n",
    "del scenario_residual_df\n",
    "del scenario_weighted_df\n",
    "del scenario_mse_df\n",
    "del scenario_chi2\n",
    "\n",
    "# make the mass column (currently a string range x.x-y.y) to be a center float value\n",
    "# this will make comparisons easier later\n",
    "residual_df[\"mass\"] = residual_df[\"mass\"].apply(lambda x: np.mean([float(i) for i in x.split(\"-\")]))\n",
    "weighted_residual_df[\"mass\"] = weighted_residual_df[\"mass\"].apply(lambda x: np.mean([float(i) for i in x.split(\"-\")]))\n",
    "mse_df[\"mass\"] = mse_df[\"mass\"].apply(lambda x: np.mean([float(i) for i in x.split(\"-\")]))\n",
    "reduced_chi2_df[\"mass\"] = reduced_chi2_df[\"mass\"].apply(lambda x: np.mean([float(i) for i in x.split(\"-\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by plotting all the residuals\n",
    "$$\n",
    "    \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{truth}} - \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{fit}}\n",
    "\\,,\n",
    "$$\n",
    "and weighted residuals\n",
    "$$\n",
    "    \\frac{\n",
    "        \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{truth}} - \\hat{H}^\\alpha(J_v,\\Lambda,J,M)^{\\text{fit}}\n",
    "    }{\n",
    "        \\sigma_{\\hat{H}}^{\\text{fit}}\n",
    "    }\\,,\n",
    "$$\n",
    "colored by scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals for every moment\n",
    "df_melted = residual_df.melt(id_vars=[\"mass\", \"scenario\"], var_name=\"moment\", value_name=\"residual\")\n",
    "grid = sns.relplot(\n",
    "    data=df_melted, x=\"mass\", y=\"residual\", col=\"moment\", \n",
    "    col_wrap=10, kind=\"line\", hue=\"scenario\", style=\"scenario\", \n",
    "    palette=SCENARIO_COLORS,\n",
    "    facet_kws={'sharey': False}\n",
    ")\n",
    "\n",
    "for ax in grid.axes.flat:\n",
    "    # remove the \"moment = \" from the title\n",
    "    subplot_title = ax.get_title()\n",
    "    ax.set_title(subplot_title.replace(\"moment = \", \"\"))\n",
    "    # set the alpha value of each marker\n",
    "    for line in ax.get_lines():\n",
    "        line.set_alpha(0.7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot the weighted residuals for every moment\n",
    "df_melted = weighted_residual_df.melt(id_vars=[\"mass\", \"scenario\"], var_name=\"moment\", value_name=\"weighted_residual\")\n",
    "grid = sns.relplot(\n",
    "    data=df_melted, x=\"mass\", y=\"weighted_residual\", col=\"moment\", \n",
    "    col_wrap=10, kind=\"line\", hue=\"scenario\", style=\"scenario\", \n",
    "    palette=SCENARIO_COLORS,\n",
    "    facet_kws={'sharey': False}\n",
    ")\n",
    "\n",
    "for ax in grid.axes.flat:\n",
    "    # remove the \"moment = \" from the title\n",
    "    subplot_title = ax.get_title()\n",
    "    ax.set_title(subplot_title.replace(\"moment = \", \"\"))\n",
    "    # set the alpha value of each marker\n",
    "    for line in ax.get_lines():\n",
    "        line.set_alpha(0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jp_chi2(fit_df, data_df, truth_df, chi2_df, scenario):\n",
    "    # grab the mass bins and widths\n",
    "    mass_bins = data_df[\"m_center\"]\n",
    "    bin_width = data_df[\"m_high\"].iloc[0] - data_df[\"m_low\"].iloc[0]\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        2, 1,\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"wspace\": 0.0, \"hspace\": 0.07},\n",
    "        height_ratios=[3, 1],\n",
    "    )\n",
    "\n",
    "    # ---AXS 0---\n",
    "    # plot the data events\n",
    "    axs[0].errorbar(\n",
    "        x=mass_bins, y=data_df[\"events\"], xerr = bin_width/2.0, yerr=data_df[\"events_err\"],\n",
    "        fmt=\"k.\", linestyle=\"\", label=\"MC Events\"\n",
    "    )\n",
    "\n",
    "    # plot fit result as gray histogram\n",
    "    axs[0].bar(\n",
    "        mass_bins, fit_df[\"detected_events\"], width=bin_width,\n",
    "        color=\"0.1\", \n",
    "        alpha=0.15, label=\"Fit Result\"\n",
    "    )\n",
    "    axs[0].errorbar(\n",
    "        x=mass_bins, y=fit_df[\"detected_events\"], yerr=fit_df[\"detected_events_err\"],\n",
    "        fmt=\",\", color=\"0.1\", alpha=0.2, markersize=0,\n",
    "    )\n",
    "\n",
    "    # plot the jp contributions\n",
    "    jp_columns = pwa_tools.get_coherent_sums(fit_df)[\"JP\"]\n",
    "    colors = mpl.colormaps[\"Dark2\"].colors\n",
    "    jp_map = {\n",
    "        \"Bkgd\": {\"color\": colors[0], \"marker\": \".\"},\n",
    "        \"0m\": {\"color\": colors[1], \"marker\": \"1\"},\n",
    "        \"1p\": {\"color\": colors[2], \"marker\": \"o\"},\n",
    "        \"1m\": {\"color\": colors[3], \"marker\": \"s\"},\n",
    "        \"2p\": {\"color\": colors[4], \"marker\": \"p\"},\n",
    "        \"2m\": {\"color\": colors[5], \"marker\": \"h\"},\n",
    "        \"3p\": {\"color\": colors[6], \"marker\": \"x\"},\n",
    "        \"3m\": {\"color\": colors[7], \"marker\": \"d\"},\n",
    "    }\n",
    "    for col in jp_columns:\n",
    "        axs[0].errorbar(\n",
    "            x=mass_bins, y=fit_df[col], yerr=fit_df[f\"{col}_err\"],\n",
    "            marker=jp_map[col][\"marker\"], linestyle=\"\", color=jp_map[col][\"color\"], markersize=6,            \n",
    "            label=pwa_tools.convert_amp_name(col)\n",
    "        )\n",
    "        axs[0].plot(\n",
    "            mass_bins, truth_df[col],\n",
    "            linestyle=\"-\", marker=\"\", color=jp_map[col][\"color\"]\n",
    "        )\n",
    "\n",
    "    axs[0].set_ylabel(f\"Events / {bin_width:.3f} GeV\", loc=\"top\")\n",
    "    axs[0].set_ylim(bottom=0.0)\n",
    "    axs[0].legend(loc=\"upper right\")\n",
    "\n",
    "    # ---AXS 1---\n",
    "    # plot the chi2\n",
    "    axs[1].plot(\n",
    "        mass_bins, chi2_df[\"chi2\"],\n",
    "        marker=\".\", linestyle=\"-\", color=\"black\"\n",
    "    )\n",
    "    axs[1].set_xlabel(r\"$\\omega\\pi^0$ inv. mass $(GeV)$\", loc=\"right\")\n",
    "    axs[1].set_ylabel(r\"$\\chi^2$ / ndf\", loc=\"center\")\n",
    "    axs[1].set_ylim(0.0, 7.0)\n",
    "\n",
    "    plt.minorticks_on()\n",
    "    plt.suptitle(f\"Scenario: {scenario}\")\n",
    "    plt.show()\n",
    "\n",
    "for scenario, df_dict in all_dfs.items():\n",
    "    chi2_subset = reduced_chi2_df[reduced_chi2_df[\"scenario\"] == scenario]      \n",
    "    plot_jp_chi2(df_dict[\"fit\"], df_dict[\"data\"], df_dict[\"truth\"], chi2_subset, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neutralb1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
